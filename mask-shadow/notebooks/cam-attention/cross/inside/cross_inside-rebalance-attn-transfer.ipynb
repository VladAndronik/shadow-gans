{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cross_inside-rebalance-attn-transfer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c6bbe9ed8d64477daadc605f6b85b671":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b896c6bef06c415ba018717500b98ad5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7bb2384888b9410c992ccbc1e09faed4","IPY_MODEL_4c7ae04b32804e2f9343ea7bd39a6d65"]}},"b896c6bef06c415ba018717500b98ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7bb2384888b9410c992ccbc1e09faed4":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d4b9ea2d051e45118e2bb73c2064c34a","_dom_classes":[],"description":"  6%","_model_name":"IntProgressModel","bar_style":"","max":155,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45cbe5b4a61d492b939ef51ae7ff397c"}},"4c7ae04b32804e2f9343ea7bd39a6d65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b76a98e1bc340819ecf3b738a36db1a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9/155 [9:05:53&lt;147:17:34, 3631.88s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7a9603b457c45078811b2e70c3c9b05"}},"d4b9ea2d051e45118e2bb73c2064c34a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"45cbe5b4a61d492b939ef51ae7ff397c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b76a98e1bc340819ecf3b738a36db1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7a9603b457c45078811b2e70c3c9b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"wOpnc8j2xLEv","colab_type":"text"},"source":["### Cross approach\n","- CAM attention for shadow removal networks only\n","- Integrating CAMs into shadow generator\n","- CAM coeff = 500. It was a better variant for ugatit option. Should be further tuned.\n","\n","Current version:\n","  - Integrating CAM inside B2A\n","  - Using learned weights in A2B to rebalance channels in B2A."]},{"cell_type":"code","metadata":{"id":"G7PsyVEfClzA","colab_type":"code","outputId":"464d632a-cfe1-4645-c1ca-b968abbd5a82","executionInfo":{"status":"ok","timestamp":1587081402429,"user_tz":-180,"elapsed":106949,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive/')\n","path_basic = 'drive/My Drive/gan_experiments'\n","os.chdir(path_basic)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2jaBlTrdCuoL","colab_type":"code","colab":{}},"source":["import torch\n","from torch.nn import init\n","from torch import nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","from torch.autograd import Variable\n","from torch.utils.tensorboard import SummaryWriter \n","\n","from skimage.filters import threshold_otsu\n","from PIL import Image\n","import cv2\n","import functools\n","import itertools\n","import numpy as np\n","import random\n","\n","import h5py\n","\n","import os\n","import sys\n","import glob\n","from tqdm import tqdm_notebook\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pickle\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eU38c1glQDp","colab_type":"text"},"source":["### Additional modules implementation"]},{"cell_type":"code","metadata":{"id":"Tbal9TpD3K2i","colab_type":"code","colab":{}},"source":["# timer utils\n","import time\n","import functools\n","def timer(f):\n","  @functools.wraps(f)\n","  def wrapper(*args, **kwargs):\n","    start_time = time.time()\n","    r = f(*args, **kwargs)\n","    duration = time.time() - start_time\n","    result = {\n","        'result':r,\n","        'time':duration\n","    }\n","    return result\n","  return wrapper"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8aJdPS2HI3h","colab_type":"code","colab":{}},"source":["class CAMAttention(nn.Module):\n","  def __init__(self, input_nc):\n","    super(CAMAttention, self).__init__()\n","    self.gap_fc = nn.utils.spectral_norm(nn.Linear(input_nc, 1, bias=False))\n","    self.gmp_fc = nn.utils.spectral_norm(nn.Linear(input_nc, 1, bias=False))\n","\n","    self.conv_1x1 = nn.utils.spectral_norm(nn.Conv2d(input_nc * 2, input_nc, 1))\n","    self.relu = nn.ReLU(True)\n","\n","\n","  def forward(self, x):\n","    # global avg. pool.\n","    gap = F.adaptive_avg_pool2d(x, 1)\n","    gap = gap.view(gap.shape[0], -1)\n","    gap_fc = self.gap_fc(gap)\n","    gap_weights = next(self.gap_fc.parameters()).unsqueeze(2).unsqueeze(3)\n","    gap_output = x * gap_weights\n","\n","    # global max. pool.\n","    gmp = F.adaptive_max_pool2d(x, 1)\n","    gmp = gmp.view(gmp.shape[0], -1)\n","    gmp_fc = self.gmp_fc(gmp)\n","    gmp_weights = next(self.gmp_fc.parameters()).unsqueeze(2).unsqueeze(3)\n","    gmp_output = x * gmp_weights\n","\n","    cam_loss = torch.cat([gap_fc, gmp_fc], axis=1)\n","    gap_gmp_output = torch.cat([gap_output, gmp_output], axis=1)\n","\n","    \n","    output = self.relu(self.conv_1x1(gap_gmp_output))\n","    attention = torch.sum(output, axis=1, keepdims=True)\n","\n","    return output, attention, cam_loss, gap_weights, gmp_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeEmGUYFIUi8","colab_type":"code","colab":{}},"source":["test = torch.rand(1,3,32,32)\n","model = CAMAttention(3)\n","out = model(test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llViHAJ2EPEn","colab_type":"text"},"source":["### Blocks"]},{"cell_type":"code","metadata":{"id":"hM7rtwdjEPAO","colab_type":"code","colab":{}},"source":["class ResidualBlock(nn.Module):\n","    \"\"\"\n","      Parameters:\n","        dilation_factor -- dilated convolution hyperparam\n","    \"\"\"\n","    def __init__(self, in_features, dilation_factor, r=4):\n","        super(ResidualBlock, self).__init__()\n","        k = 3\n","        d = dilation_factor\n","        pad = int(((k - 1) * (d - 1) + k - 1) / 2)\n","\n","        conv_block = [  nn.ReflectionPad2d(pad),\n","                        nn.utils.spectral_norm(nn.Conv2d(in_features, in_features, 3, dilation=d)),\n","                        nn.InstanceNorm2d(in_features),\n","                        nn.ReLU(inplace=True),\n","                      \n","                        nn.ReflectionPad2d(pad),\n","                        nn.utils.spectral_norm(nn.Conv2d(in_features, in_features, 3, dilation=d)),\n","                        nn.InstanceNorm2d(in_features)\n","                      ]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class Generator_S2F(nn.Module):\n","    \"\"\"\n","      attn_idx -- index of the bottleneck block onwards to place attention block\n","    \"\"\"\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=9,\n","                 dilation_factors=[1,1,1,2,4,8,16,1,1],\n","                 attn_idx=4):\n","        super(Generator_S2F, self).__init__()\n","        assert len(dilation_factors) == n_residual_blocks\n","\n","        downsample = []\n","        bottleneck_1 = []  # modules before attention\n","        bottleneck_2 = []  # modules after attention\n","        upsample = []\n","\n","        # Initial convolution block\n","        downsample += [   nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(input_nc, 64, 7)),\n","                    nn.InstanceNorm2d(64),\n","                    nn.ReLU(inplace=True) ]\n","\n","        # Downsampling\n","        in_features = 64\n","        out_features = in_features*2\n","        for _ in range(2):\n","            downsample += [  nn.utils.spectral_norm(nn.Conv2d(in_features, out_features, 3, stride=2, padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features*2\n","\n","        # Residual blocks\n","        # Paste attn module before dilation growth\n","        for i in range(attn_idx):\n","          d = dilation_factors[i]\n","          bottleneck_1 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        self.cam_attn = CAMAttention(in_features)\n","        for i in range(attn_idx, n_residual_blocks):\n","          d = dilation_factors[i]\n","          bottleneck_2 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        # Upsampling\n","        out_features = in_features//2\n","        for _ in range(2):\n","            upsample += [  nn.utils.spectral_norm(nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features//2\n","\n","        # Output layer\n","        upsample += [  nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(64, output_nc, 7)) ]\n","                    #nn.Tanh() ]\n","\n","        self.downsample = nn.Sequential(*downsample)\n","        self.bottleneck_1 = nn.Sequential(*bottleneck_1)\n","        self.bottleneck_2 = nn.Sequential(*bottleneck_2)\n","        self.upsample = nn.Sequential(*upsample)\n","\n","    def forward(self, x):\n","      downsampled = self.downsample(x)\n","\n","      bottleneck_1 = self.bottleneck_1(downsampled)\n","      cam_output, cam_attn, cam_loss, cam_gap_weights, cam_gmp_weights = self.cam_attn(bottleneck_1)\n","      bottleneck_2 = self.bottleneck_2(cam_output)\n","\n","      output = self.upsample(bottleneck_2)\n","      output = (output + x).tanh() #(min=-1, max=1) #just learn a residual\n","      return output, cam_attn, cam_loss, cam_gap_weights, cam_gmp_weights\n","\n","\n","class Generator_F2S(nn.Module):\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=9,\n","                 dilation_factors=[1,1,1,2,4,8,16,1,1],\n","                 attn_idx=-1):\n","        super(Generator_F2S, self).__init__()\n","        assert len(dilation_factors) == n_residual_blocks\n","        self.attn_idx = attn_idx\n","        downsample = []\n","        if self.attn_idx > 0:\n","          bottleneck_1 = []  # modules before attention\n","          bottleneck_2 = []  # modules after attention\n","        else:\n","          bottleneck = []\n","\n","        upsample = []\n","\n","        # Initial convolution block\n","        downsample += [   nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(input_nc+1, 64, 7)), # + mask\n","                    nn.InstanceNorm2d(64),\n","                    nn.ReLU(inplace=True) ]\n","\n","        # Downsampling\n","        in_features = 64\n","        out_features = in_features*2\n","        for _ in range(2):\n","            downsample += [  nn.utils.spectral_norm(nn.Conv2d(in_features, out_features, 3, stride=2, padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features*2\n","\n","        # Residual blocks\n","        # Paste attn module before dilation growth\n","        for i in range(attn_idx):\n","          d = dilation_factors[i]\n","          bottleneck_1 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        self.conv_1x1 = nn.utils.spectral_norm(nn.Conv2d(in_features * 2, in_features, 1))\n","        self.relu = nn.ReLU(True)\n","\n","        for i in range(attn_idx, n_residual_blocks):\n","          d = dilation_factors[i]\n","          bottleneck_2 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        # Upsampling\n","        out_features = in_features//2\n","        for _ in range(2):\n","            upsample += [  nn.utils.spectral_norm(nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features//2\n","\n","        # Output layer\n","        upsample += [  nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(64, output_nc, 7)) ]\n","                    #nn.Tanh() ]\n","\n","        self.downsample = nn.Sequential(*downsample)\n","\n","        self.bottleneck_1 = nn.Sequential(*bottleneck_1)\n","        self.bottleneck_2 = nn.Sequential(*bottleneck_2)\n","\n","        self.upsample = nn.Sequential(*upsample)\n","\n","        model = downsample + bottleneck_1 + bottleneck_2 + upsample\n","        self.model = nn.Sequential(*model)\n","\n","\n","    def forward(self, x, mask, weights):\n","      if weights[0] is None:\n","        # if identity\n","        return (self.model(torch.cat((x, mask), axis=1)) + x).tanh()\n","\n","      downsampled = self.downsample(torch.cat((x, mask), 1))\n","\n","      bottleneck_1_output = self.bottleneck_1(downsampled)\n","      # cam attention integrating. rebalancing.      \n","      gap_weights, gmp_weights = weights\n","      gap_output = bottleneck_1_output * gap_weights\n","      gmp_output = bottleneck_1_output * gmp_weights\n","      gap_gmp_output = torch.cat([gap_output, gmp_output], axis=1)\n","      cam_output = self.relu(self.conv_1x1(gap_gmp_output))  # return initial number of channels\n","      cam_attn = torch.sum(cam_output, axis=1, keepdims=True)  # for qualitative estimation\n","\n","      bottleneck_2_output = self.bottleneck_2(cam_output)\n","      output = self.upsample(bottleneck_2_output)\n","      output = (output + x).tanh()  # (min=-1, max=1) #just learn a residual\n","      return output, cam_attn\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_nc, attn=True):\n","        super(Discriminator, self).__init__()\n","\n","        # A bunch of convolutions one after another\n","        self.attn = attn\n","        model = []\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(input_nc, 64, 4, stride=2, padding=1)),\n","                 nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),\n","                  nn.InstanceNorm2d(128),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),\n","                  nn.InstanceNorm2d(256),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, padding=1)),\n","                  nn.InstanceNorm2d(512),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        # CAM layers\n","        if self.attn:\n","          self.cam_attn = CAMAttention(512)\n","          self.conv_output = nn.utils.spectral_norm(nn.Conv2d(512, 1, 4, padding=1))\n","        else:\n","          model += [nn.utils.spectral_norm(nn.Conv2d(512, 1, 4, padding=1))]\n","\n","        self.model = nn.Sequential(*model)\n","\n","\n","    def forward(self, x):\n","      x = self.model(x)\n","      if not self.attn:\n","        # Average pooling and flatten\n","        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1) #global avg pool\n","\n","      cam_output, cam_attn, cam_loss,_,_ = self.cam_attn(x)\n","      output = self.conv_output(cam_output)\n","      output = F.avg_pool2d(output, output.size()[2:]).view(output.size()[0], -1) #global avg pool\n","      return output, cam_attn, cam_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-b93raDXeCd","colab_type":"code","colab":{}},"source":["test = torch.rand(1,3,256,256)\n","gab = Generator_S2F(3,3,attn_idx=4)\n","gba_attn = Generator_S2F(3,3,attn_idx=4)\n","gba_noattn = Generator_S2F(3,3,attn_idx=-1)\n","d_attn = Discriminator(3, attn=True)\n","d_noattn = Discriminator(3, attn=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Jof0vreXd07","colab_type":"code","colab":{}},"source":["gab_out, gab_attn, gab_cam = gab(test)\n","gba_attn_out, gba_attn_attn, gba_attn_cam = gba_attn(test)\n","gba_noattn_out = gba_noattn(test)\n","\n","d_attn_out, d_attn_attn, d_attn_cam = d_attn(test)\n","d_noattn_out = d_noattn(test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhlQi8gIZ3Ke","colab_type":"code","outputId":"24f66f9e-6ad3-4a3f-85f2-8ca57d32e363","executionInfo":{"status":"ok","timestamp":1585252370908,"user_tz":-120,"elapsed":1515,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["gab_attn"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[7.1170, 6.2092, 6.4364,  ..., 5.8054, 6.1612, 5.3988],\n","          [6.5128, 6.3957, 5.9861,  ..., 6.6703, 7.8026, 6.6175],\n","          [5.8081, 5.6422, 7.0622,  ..., 5.9779, 6.3982, 6.6766],\n","          ...,\n","          [5.1379, 6.5081, 5.8854,  ..., 6.8845, 6.2883, 6.3895],\n","          [6.9261, 6.6220, 6.6822,  ..., 7.7174, 4.6873, 8.0985],\n","          [5.0194, 6.5406, 6.7473,  ..., 6.2798, 6.0392, 5.7299]]]],\n","       grad_fn=<SumBackward1>)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"w4o0Q56KWb1E","colab_type":"code","outputId":"724e1f36-6226-4a6b-9638-5b1b18951752","executionInfo":{"status":"ok","timestamp":1584915955784,"user_tz":-120,"elapsed":1268,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["d_noattn_out"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.2523]], grad_fn=<ViewBackward>)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"5fK-DCFjaike","colab_type":"code","colab":{}},"source":["gab = Generator_S2F(3,3,attn_idx=4)\n","gab.to(device)\n","_=gab.apply(weights_init_normal)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnGgA_zUa0xW","colab_type":"code","colab":{}},"source":["tr = [\n","      transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","      transforms.ToTensor(),\n","      transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n","]\n","dt = ImageDatasetValidation(root_dir, transforms_=tr, mode='val')\n","img = dt[0]['A']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"voYqhC7jdx65","colab_type":"code","colab":{}},"source":["img = img.to(device).unsqueeze(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnfLzfYBbWIV","colab_type":"code","colab":{}},"source":["_, attn, _ = gab(img)\n","attn.max(), attn.min()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3GLV8BUeBD3","colab_type":"code","outputId":"62513e15-9fe4-43cf-9e83-3b7aad57e147","executionInfo":{"status":"ok","timestamp":1585253562491,"user_tz":-120,"elapsed":1008,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n","print(attn_norm.max(), attn_norm.min())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(1., device='cuda:0', grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ReXCsD-obV-R","colab_type":"code","outputId":"f3577acd-1cbf-4e0c-b20a-1c1695aee48e","executionInfo":{"status":"ok","timestamp":1585253610709,"user_tz":-120,"elapsed":1308,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["attn_norm = (attn_norm - 0.5) / 0.5\n","print(attn_norm.max(), attn_norm.min())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(1., device='cuda:0', grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', grad_fn=<MinBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"50QAnrILenvk","colab_type":"code","colab":{}},"source":["attn_array = attn.data.squeeze(0).squeeze(0).cpu().numpy()\n","attn_norm_array = attn_norm.data.squeeze(0).squeeze(0).cpu().numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGRdX_0me0aG","colab_type":"code","outputId":"8fdffc99-dd1c-4f81-ecab-3e2744af67d7","executionInfo":{"status":"ok","timestamp":1585253775537,"user_tz":-120,"elapsed":1746,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":264}},"source":["import seaborn as sns\n","plt.subplots(figsize=(15,8))\n","plt.subplot(2,2,1)\n","sns.distplot(attn_array.flatten())\n","\n","plt.subplot(2,2,2)\n","sns.distplot(attn_norm_array.flatten())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fd736fd05c0>"]},"metadata":{"tags":[]},"execution_count":52},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA3AAAADlCAYAAAASqRW6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 1080x576 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"L3lYJ6CkYFXy","colab_type":"code","colab":{}},"source":["def cam(x, size = 256):\n","    x = x - np.min(x)\n","    cam_img = x / np.max(x)\n","    cam_img = np.uint8(255 * cam_img)\n","    cam_img = cv2.resize(cam_img, (size, size))\n","    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n","    return cam_img / 255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hv8Uc0N3YFOx","colab_type":"code","colab":{}},"source":["gab_attn = cam(gab_attn.detach().numpy().squeeze())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tc-v0TIPGM7u","colab_type":"code","colab":{}},"source":["plt.imshow(gab_attn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YJi52j7GIi1","colab_type":"text"},"source":["### Buffers"]},{"cell_type":"code","metadata":{"id":"Eek1JNenFvmu","colab_type":"code","colab":{}},"source":["class ReplayBuffer():\n","    def __init__(self, max_size=50):\n","        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0,1) > 0.5:\n","                    i = random.randint(0, self.max_size-1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))\n","\n","class QueueMask():\n","    def __init__(self, length):\n","        self.max_length = length\n","        self.queue = []\n","\n","    def insert(self, mask, weights):\n","        if self.queue.__len__() >= self.max_length:\n","            self.queue.pop(0)\n","        self.queue.append((mask, weights))\n","\n","    def preprocess(self, mask, attn):\n","      # attention resizing\n","      attn = self.prepare_attn(attn)\n","      output = torch.cat([mask, attn], axis=1)\n","      return output\n","\n","    def prepare_attn(self, attn):\n","      # resize the image to (image_size, image_size)\n","      attn = attn.data.squeeze().cpu().numpy()\n","      attn = cv2.resize(attn, (image_size, image_size),interpolation=cv2.INTER_CUBIC)\n","      attn = torch.cuda.FloatTensor(attn).unsqueeze(0).unsqueeze(0)\n","\n","      # normalize values to [-1,1]\n","      attn = (attn - attn.min()) / (attn.max() - attn.min())\n","      attn = (attn - 0.5) / 0.5\n","      return attn\n","\n","    def rand_item(self):\n","        assert self.queue.__len__() > 0, 'Error! Empty queue!'\n","        return self.queue[np.random.randint(0, self.queue.__len__())]\n","\n","    def last_item(self):\n","        assert self.queue.__len__() > 0, 'Error! Empty queue!'\n","        return self.queue[self.queue.__len__()-1]\n","        \n","\n","class ImageDatasetHDF5(Dataset):\n","    def __init__(self, path_hdf5, transforms_=None):\n","        self.transform = transforms.Compose(transforms_)\n","        self.files = h5py.File(path_hdf5, \"r\")\n","        self.domain_A = 'domain_A'\n","        self.domain_B = 'domain_B'\n","\n","        self.length_A = self.files[self.domain_A].shape[0]\n","        self.length_B = self.files[self.domain_B].shape[0]\n","\n","    @timer\n","    def __getitem__(self, index):\n","        item_A = self.transform(self.files[self.domain_A][index % self.length_A])\n","        item_B = self.transform(self.files[self.domain_B][random.randint(0, self.length_B - 1)])\n","        return {'A': item_A, 'B': item_B}\n","\n","    def __len__(self):\n","        return max(self.length_A, self.length_B)\n","\n","\n","class ImageDatasetValidation(Dataset):\n","    def __init__(self, root, paired=False, transforms_=None, mode='val', mask=False):\n","        self.transform = transforms.Compose(transforms_)\n","        self.paired = paired\n","        self.files_A = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_A', '*.png')))\n","        self.files_B = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_C', '*.png')))\n","        self.mask = mask\n","        if self.mask:\n","          self.transform_mask = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","                                                    transforms.ToTensor()])\n","          self.files_mask = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_B', '*.png')))\n","\n","    def __getitem__(self, index):\n","        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n","        \n","        if self.paired:\n","          item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n","          if self.mask:\n","            item_mask = self.transform_mask(Image.open(self.files_mask[index % len(self.files_mask)]))\n","            return {'A': item_A, 'B': item_B, 'mask':item_mask}  \n","          return {'A': item_A, 'B': item_B}  \n","        item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n","        return {'A': item_A, 'B': item_B}\n","\n","    def __len__(self):\n","        return max(len(self.files_A), len(self.files_B))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bcS40lq_tcV","colab_type":"code","colab":{}},"source":["# V0.1.0 num params\n","V010_G = 45.512\n","V010_D = 11.058\n","V010_S = 56.57"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWucd1lR_ZiX","colab_type":"code","outputId":"6553c221-0daf-4d96-86c0-f5283f73699a","executionInfo":{"status":"ok","timestamp":1585179182381,"user_tz":-120,"elapsed":7016,"user":{"displayName":"Vlad Andronik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhbMGxHkjokCi9rObSClj5lf1eT5AbYqBaXBHWUw=s64","userId":"00447066830077822735"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# summary of the models\n","def num_params(model):\n","  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","  params = sum([np.prod(p.size()) for p in model_parameters])\n","  return params\n","\n","dL = Discriminator(3, attn=True)\n","g = Generator_S2F(3,3)\n","\n","V011_D = round(num_params(dL) * 2 / 1e6, 3) * 2\n","V011_G = round(num_params(g) * 2 / 1e6, 3) * 2\n","V011_S = round(V011_D + V011_G, 3)\n","\n","print('---- Summary models ----')\n","print(\"Number of parameters (in millions):\")\n","print(\"{:10}{:10}{:20}\".format(\"D\", 'G', \"Overall\"))\n","print(\"{:10}{:10}{:20}\".format(str(V011_D), str(V011_G), str(V011_S)))\n","\n","print(f\"GAIN PARAMS: {round((V011_S / V010_S - 1) * 100, 3)}%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["---- Summary models ----\n","Number of parameters (in millions):\n","D         G         Overall             \n","13.162    46.04     59.202              \n","GAIN PARAMS: 4.653%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ih5jjEJIGPwI","colab_type":"text"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"KsAG2hLaFyt5","colab_type":"code","colab":{}},"source":["to_pil = transforms.ToPILImage()\n","to_gray = transforms.Grayscale(num_output_channels=1)\n","\n","\n","def mask_generator(shadow, shadow_free):\n","\tim_f = to_gray(to_pil(((shadow_free.data.squeeze(0) + 1.0) * 0.5).cpu()))\n","\tim_s = to_gray(to_pil(((shadow.data.squeeze(0) + 1.0) * 0.5).cpu()))\n","\n","\tdiff = (np.asarray(im_f, dtype='float32')- np.asarray(im_s, dtype='float32')) # difference between shadow image and shadow_free image\n","\tL = threshold_otsu(diff)\n","\tmask = torch.tensor((np.float32(diff >= L)-0.5)/0.5).unsqueeze(0).unsqueeze(0).cuda() #-1.0:non-shadow, 1.0:shadow\n","\tmask.requires_grad = False\n","\treturn mask\n","\n","\n","\n","def tensor2image(tensor):\n","    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n","    if image.shape[0] == 1:\n","        image = np.tile(image, (3,1,1))\n","    return image.astype(np.uint8)\n","\n","def cam(x, size = 256):\n","    x = x - np.min(x)\n","    cam_img = x / np.max(x)\n","    cam_img = np.uint8(255 * cam_img)\n","    cam_img = cv2.resize(cam_img, (size, size))\n","    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n","    return cam_img\n","\n","\n","class LambdaLR():\n","    def __init__(self, n_epochs, offset, decay_start_epoch):\n","        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n","        self.n_epochs = n_epochs\n","        self.offset = offset\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch):\n","        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n","\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRZ9EoJlwsnM","colab_type":"text"},"source":["### Evaluators"]},{"cell_type":"code","metadata":{"id":"TkuvuH0W4x7O","colab_type":"code","colab":{}},"source":["def read_paths(path):\n","  with open(path, 'rb') as f:\n","    paths = pickle.load(f)\n","  return paths\n","\n","def mkdir(path):\n","  try:\n","    os.mkdir(path)\n","  except FileExistsError as e:\n","    pass\n","\n","def save_test(A_path, B_path, save_path):\n","  start_time = time.time()\n","  A_paths = read_paths(A_path)\n","  B_paths = read_paths(B_path)\n","  \n","  assert len(A_paths) == len(B_paths)\n","  # read and preprocess the test images\n","  os.makedirs(save_path, exist_ok=True)\n","\n","  mkdir(os.path.join(save_path, 'A_B'))\n","  mkdir(os.path.join(save_path, 'B_A'))\n","\n","  mkdir(os.path.join(save_path, 'masks'))\n","  mkdir(os.path.join(save_path, 'attn_A_B'))\n","  mkdir(os.path.join(save_path, 'attn_B_A'))\n","\n","  # load the model\n","  netG_A2B = Generator_S2F(input_nc, output_nc, attn_idx=attn_idx_GAB).to(device)\n","  netG_B2A = Generator_F2S(input_nc, output_nc, attn_idx=attn_idx_GBA).to(device)\n","\n","\n","  # load latest checkpoint\n","  netG_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_A2B.pth')))\n","  netG_B2A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_B2A.pth')))\n","\n","  # turn the validation mode\n","  netG_A2B.eval()\n","  netG_B2A.eval()\n","\n","  # input tensors\n","  Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n","  input_A = Tensor(batch_size, input_nc, image_size, image_size, 3)\n","  input_B = Tensor(batch_size, output_nc, image_size, image_size, 3)\n","\n","  # input transformations\n","  img_transforms = transforms.Compose([\n","                                       transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n","  ])\n","  to_pil = transforms.ToPILImage()\n","\n","  image_queue = QueueMask(length=mask_queue_size)\n","  for i,(path_A, path_B) in enumerate(zip(A_paths, B_paths)):\n","    image_A = Image.open(path_A).convert(\"RGB\")\n","    image_B = Image.open(path_B).convert(\"RGB\")\n","\n","    im_A = (img_transforms(image_A).unsqueeze(0)).to(device)\n","    im_B = (img_transforms(image_B).unsqueeze(0)).to(device)\n","\n","  \n","\n","    # generate A -> B\n","    A_B, attn_A_B, _, gap_weight, gmp_weight = netG_A2B(im_A)\n","    w,h = image_A.size\n","\n","    current_mask = mask_generator(A_B, im_A)\n","    image_queue.insert(current_mask, (gap_weight, gmp_weight))\n","    A_B = .5 * (A_B + 1)\n","    A_B = np.array((to_pil(A_B.data.squeeze(0).cpu())))\n","    attn_A_B = cam(attn_A_B.data.squeeze().cpu().numpy(), size=w)\n","    Image.fromarray(A_B).save(os.path.join(save_path, 'A_B', 'A_B_{}.jpg'.format(i)))\n","    Image.fromarray(attn_A_B).save(os.path.join(save_path, 'attn_A_B', 'A_B_{}.jpg'.format(i)))\n","\n","    # generate B -> A\n","    mask, weights = image_queue.rand_item()\n","    B_A, attn_B_A = netG_B2A(im_B, mask, weights)\n","    w,h = image_B.size\n","\n","    B_A = .5 * (B_A + 1)\n","    B_A = np.array((to_pil(B_A.data.squeeze(0).cpu())))\n","    Image.fromarray(B_A).save(os.path.join(save_path, 'B_A', 'B_A_{}.jpg'.format(i)))\n","\n","    mask_cpu = np.array((to_pil(.5 * (current_mask.data + 1).squeeze(0).cpu())))\n","    Image.fromarray(mask_cpu).save(os.path.join(save_path, 'masks', 'mask_{}.jpg'.format(i)))\n","\n","    attn_B_A = cam(attn_B_A.data.squeeze().cpu().numpy(), size=w)\n","    Image.fromarray(attn_B_A).save(os.path.join(save_path, 'attn_B_A', 'B_A_{}.jpg'.format(i)))\n","\n","  print(\"---- Inference finished. Time : {} ----\".format(time.time() - start_time).upper())\n","  return image_queue\n","\n","\n","def save_images(image_paths, save_path, domain):\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    pass\n","\n","  for i in range(len(image_paths)):\n","    img = plt.imread(image_paths[i])\n","    plt.imsave(os.path.join(save_path,'{}_{}.jpg'.format(domain, i)), img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7c7noBkq0TmC","colab_type":"code","colab":{}},"source":["from skimage import color, io\n","def evaluate(image_true, image_pred, mask):\n","    mask = torch.where(mask.expand(-1, 3, -1, -1))\n","    image_true_shreg = image_true[mask].squeeze().detach().cpu().numpy()\n","    image_pred_shreg = image_pred[mask].squeeze().detach().cpu().numpy()\n","    image_true_shreg = 127.5 * (image_true_shreg + 1)\n","    image_pred_shreg = 127.5 * (image_pred_shreg + 1)\n","\n","    image_true = convert_image_array(image_true)\n","    image_pred = convert_image_array(image_pred)\n","\n","    image_true = color.rgb2lab(image_true)\n","    image_pred = color.rgb2lab(image_pred)\n","\n","\n","    rmse_whole = np.sqrt(np.square(image_true - image_pred))\n","    rmse_shreg = np.sqrt(np.square(image_true_shreg - image_pred_shreg))\n","    return np.mean(rmse_whole), np.mean(rmse_shreg)\n","\n","\n","def convert_image_array(image):\n","  image = 127.5 * (image.squeeze(0) + 1)\n","  image = image.permute(1,2,0)\n","  \n","  return image.data.cpu().numpy().astype(np.uint8)\n","\n","\n","def decode(image):\n","  image = 127.5 * (image + 1)\n","  return image.data.cpu()\n","\n","def iou_coef(y_true, y_pred, smooth=1):\n","  y_true = y_true.data.cpu()\n","  y_pred=  decode(y_pred)\n","  intersection = torch.sum(y_true * y_pred, axis=[1,2,3])\n","  union = torch.sum(y_true, axis=[1,2,3]) + torch.sum(y_pred, axis=[1,2,3]) - intersection\n","\n","  return torch.mean((intersection + smooth) / (union + smooth), axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewGd-NfG8m3N","colab_type":"code","outputId":"4405ab32-b3d3-4eca-d896-05c9d8f7841a","executionInfo":{"status":"ok","timestamp":1585251785543,"user_tz":-120,"elapsed":5876,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["im1, im2 = torch.rand(1,3,32,32), torch.rand(1,3,32,32)\n","mask = torch.randint(0,2,size=(1,1,32,32))\n","evaluate(im1, im2, mask)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(22.03116014789175, 42.663494)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"hXJjtqf5IL_o","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"sR8ZGJ5Z5_PK","colab_type":"code","colab":{}},"source":["# _ = [os.remove(os.path.join(summary_dir, path)) for path in os.listdir(summary_dir)]\n","# _ = [os.remove(os.path.join(images_dir, path)) for path in os.listdir(images_dir)]\n","# _ = [os.remove(os.path.join(checkpoint_dir, path)) for path in os.listdir(checkpoint_dir)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOpXykMJIK_a","colab_type":"code","colab":{}},"source":["filename_ = 'ins_reb'\n","checkpoint_dir = f'mask_shadow_gan/output/checkpoints/cam_cross/checkpoints_{filename_}_v0.0.1/1/'\n","images_dir = f'mask_shadow_gan/output/images/cam_cross/images_{filename_}_v0.0.1/1/'\n","summary_dir = f'mask_shadow_gan/output/summary/cam_cross/summary_{filename_}_v0.0.1/1/'\n","\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","os.makedirs(images_dir, exist_ok=True)\n","os.makedirs(summary_dir, exist_ok=True)\n","\n","root_dir = 'data/ISTD_Dataset/'\n","hdf5_file = 'data/ISTD_Dataset/hdf5_dataset/data_istd.h5'\n","\n","# validation pathes\n","A_dir_inf = 'mask_shadow_gan/output/results/test_set_meta/test_paths/ISTD/shadow_path.pickle'\n","B_dir_inf = 'mask_shadow_gan/output/results/test_set_meta/test_paths/ISTD/free_path.pickle'\n","results_dir = f'mask_shadow_gan/output/results/cam_cross/{filename_}_v.0.0.1/1'\n","\n","\n","\n","load_model = True\n","batch_size=1\n","image_size=256\n","ngf=64\n","ndf=64\n","\n","lambda1=10\n","lambda2=10\n","identity_lambda = 0.5\n","learning_rate=2e-4\n","lr_D = 4e-4  # TTUR\n","lr_G = 1e-4  # TTUR\n","\n","beta1=.5\n","mask_queue_size=50\n","slope=0.2\n","stddev=0.02\n","\n","input_nc=3\n","output_nc=3\n","\n","n_res_blocks=9\n","dilation_A2B = [1,1,1,1,1,2,2,4,8]\n","dilation_B2A = [1,1,1,1,1,2,2,4,8]\n","REAL_LABEL=0.9  # label smoothing\n","\n","device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","n_epochs=200\n","decay_epoch=100\n","epoch=0\n","img_snapshot=500\n","model_snapshot=5\n","log_snapshot=10\n","val_snapshot=100\n","\n","coef_identity = 5\n","coef_cycle = 10\n","coef_adv = 1\n","coef_cam = 500\n","\n","attn_idx_GAB = 5\n","attn_idx_GBA = 5\n","attn_DA = False\n","attn_DB = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsP_efe1yU6u","colab_type":"code","colab":{}},"source":["class MaskShadowGAN(object):\n","  def __init__(self):\n","    pass\n","\n","  def build(self):\n","    ###### Definition of variables ######\n","    # Networks\n","    print(f\"------------------- Definition of variables -------------------\")\n","    \n","    self.netG_A2B = Generator_S2F(input_nc, output_nc, \n","                                  n_residual_blocks=n_res_blocks,\n","                                  dilation_factors=dilation_A2B, attn_idx=attn_idx_GAB)  # shadow to shadow_free\n","    self.netG_B2A = Generator_F2S(output_nc, input_nc,\n","                                  n_residual_blocks=n_res_blocks,\n","                                  dilation_factors=dilation_B2A, attn_idx=attn_idx_GBA)  # shadow_free to shadow\n","    self.netD_A = Discriminator(input_nc, attn=attn_DA)\n","    self.netD_B = Discriminator(output_nc, attn=attn_DB)\n","\n","\n","    self.netG_A2B.cuda()\n","    self.netG_B2A.cuda()\n","    self.netD_A.cuda()\n","    self.netD_B.cuda()\n","\n","    self.netG_A2B.apply(weights_init_normal)\n","    self.netG_B2A.apply(weights_init_normal)\n","    self.netD_A.apply(weights_init_normal)\n","    self.netD_B.apply(weights_init_normal)\n","\n","    # Lossess\n","    self.criterion_GAN = torch.nn.MSELoss().to(device)  # lsgan\n","    # criterion_GAN = torch.nn.BCEWithLogitsLoss() #vanilla\n","    self.criterion_cycle = torch.nn.L1Loss().to(device)\n","    self.criterion_identity = torch.nn.L1Loss().to(device)\n","    self.criterion_cam = nn.BCEWithLogitsLoss().to(device)\n","\n","    # Optimizers & LR schedulers\n","    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A2B.parameters(), self.netG_B2A.parameters()),\n","                    lr=lr_G, betas=(0.5, 0.999))\n","    self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=lr_D, betas=(0.5, 0.999))\n","    self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=lr_D, betas=(0.5, 0.999))\n","\n","    self.lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(self.optimizer_G,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    self.lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D_A,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    self.lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D_B,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    \n","    self.Tensor = torch.cuda.FloatTensor\n","    self.input_A = self.Tensor(batch_size, input_nc, image_size, image_size)\n","    self.input_B = self.Tensor(batch_size, output_nc, image_size, image_size)\n","    self.target_real = Variable(self.Tensor(batch_size).fill_(REAL_LABEL), requires_grad=False)\n","    self.target_fake = Variable(self.Tensor(batch_size).fill_(0.0), requires_grad=False)\n","    self.mask_non_shadow = Variable(self.Tensor(batch_size, 1, image_size, image_size).fill_(-1.0), requires_grad=False) #-1.0 non-shadow\n","\n","    self.fake_A_buffer = ReplayBuffer()\n","    self.fake_B_buffer = ReplayBuffer()\n","\n","    # Dataset loader\n","    self.transforms_ = [\n","            # transforms.ToPILImage(),\n","            transforms.Resize(int(image_size * 1.12), Image.BICUBIC),\n","            transforms.RandomCrop(image_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","\n","    transforms_test = [\n","      transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","      transforms.ToTensor(),\n","      transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n","    ]\n","\n","    # self.dataloader = DataLoader(ImageDatasetHDF5(hdf5_file, transforms_=self.transforms_),\n","    #                                           batch_size=batch_size, shuffle=True, num_workers=1)\n","    \n","    self.dataloader_train = DataLoader(ImageDatasetValidation(root_dir, paired=False, \n","                                                              transforms_=self.transforms_, \n","                                                              mode='train'), \n","                                    batch_size=1, shuffle=False, num_workers=os.cpu_count())\n","    self.dataloader_val = DataLoader(ImageDatasetValidation(root_dir, paired=True, \n","                                                            transforms_=transforms_test, \n","                                                            mode='val', mask=True ), \n","                                    batch_size=1, shuffle=False, num_workers=os.cpu_count())\n","    \n","\n","    \"\"\"Summary writing to tensorboard\"\"\"\n","    self.writer = SummaryWriter(log_dir=summary_dir)\n","    return self\n","    \n","\n","  def train(self):\n","    step = 0\n","    epoch = 0\n","    if load_model:\n","      print(\"Resume training\")\n","      epoch, step = self.load_model()\n","      # epoch+=1\n","\n","    to_pil = transforms.ToPILImage()\n","    mask_queue =  QueueMask(self.dataloader_train.__len__()/4)\n","\n","    ###### Training ######\n","    print(f\"------------------- Start Training -------------------\")\n","    epoch_cp = epoch\n","    try:\n","      for ep in tqdm_notebook(range(epoch, n_epochs), total=n_epochs-epoch):\n","        for i, batch in enumerate(self.dataloader_train):\n","          # Set model input\n","          # batch = _batch['result']\n","          # time_batch = _batch['time']\n","          real_A = Variable(self.input_A.copy_(batch['A']))\n","          real_B = Variable(self.input_B.copy_(batch['B']))\n","\n","          ###### Generators A2B and B2A ######\n","          start_time = time.time()\n","          self.optimizer_G.zero_grad()\n","          \n","          # forward\n","          fake_A2B, fake_A2B_attn, fake_A2B_cam, fake_A2B_gap, fake_A2B_gmp = self.netG_A2B(real_A)\n","          mask_queue.insert(mask_generator(real_A, fake_A2B), (fake_A2B_gap, fake_A2B_gmp))\n","          fake_B2A,_ = self.netG_B2A(real_B, *mask_queue.rand_item())\n","\n","          # cycle\n","          fake_A2B2A,_ = self.netG_B2A(fake_A2B, *mask_queue.last_item())\n","          fake_B2A2B, _, _, _,_ = self.netG_A2B(fake_B2A)\n","\n","          # identity\n","          fake_B2B,_,fake_B2B_cam, _,_ = self.netG_A2B(real_B)  # do not remove shadow if nothing to remove\n","          fake_A2A = self.netG_B2A(real_A, self.mask_non_shadow, (None, None))  # do not generate shadow if nothing to generate\n","\n","          # disc preds\n","          pred_fake_A2B, _, pred_fake_A2B_cam = self.netD_B(fake_A2B)\n","          pred_fake_B2A = self.netD_A(fake_B2A)\n","\n","          # losses\n","          # adversarial\n","          loss_GAN_A2B = self.criterion_GAN(pred_fake_A2B, self.target_real)\n","          loss_GAN_cam_A2B = self.criterion_GAN(pred_fake_A2B_cam, \n","                                                (torch.ones_like(pred_fake_A2B_cam) * REAL_LABEL).to(device))\n","\n","          loss_GAN_B2A = self.criterion_GAN(pred_fake_B2A, self.target_real)\n","\n","\n","          # cam loss as binary classification problem. \n","          # assign higher probs to images from current domain than to the other ones.\n","          # cam loss for discriminator is the same as its adv. counterpart\n","          loss_cam_A2B = self.criterion_cam(fake_A2B_cam, torch.ones_like(fake_A2B_cam).to(device)) + \\\n","            self.criterion_cam(fake_B2B_cam, torch.zeros_like(fake_B2B_cam).to(device))\n","          \n","          # identity loss\n","          loss_identity_A = self.criterion_identity(fake_A2A, real_A)\n","          loss_identity_B = self.criterion_identity(fake_B2B, real_B)\n","\n","          # cycle loss\n","          loss_cycle_BAB = self.criterion_cycle(real_B, fake_B2A2B)\n","          loss_cycle_ABA = self.criterion_cycle(real_A, fake_A2B2A)\n","\n","          loss_G = coef_adv * (loss_GAN_A2B + loss_GAN_cam_A2B + loss_GAN_B2A) + \\\n","            coef_cam * (loss_cam_A2B) + coef_cycle * (loss_cycle_ABA + loss_cycle_BAB) + \\\n","            coef_identity * (loss_identity_A + loss_identity_B)\n","\n","          start_time = time.time()\n","          loss_G.backward()\n","          self.optimizer_G.step()\n","          time_G_backward = time.time() - start_time\n","\n","          ###################################\n","\n","          ###### Discriminator A ######\n","          start_time = time.time()\n","          self.optimizer_D_A.zero_grad()\n","\n","          pred_real = self.netD_A(real_A)\n","          fake_B2A = self.fake_A_buffer.push_and_pop(fake_B2A)\n","          pred_fake = self.netD_A(fake_B2A.detach())\n","\n","          # Real loss\n","          loss_D_A_real = self.criterion_GAN(pred_real, self.target_real)\n","          \n","          # Fake loss\n","          loss_D_A_fake = self.criterion_GAN(pred_fake, self.target_fake)\n","          \n","          # Total loss. Decide whether to scale by 0.5(0.25): Better to add for consistency with prev. results\n","          loss_D_A = coef_adv * 0.5 * (loss_D_A_real + loss_D_A_fake)\n","\n","          time_DA_forward = time.time() - start_time\n","          \n","          start_time = time.time()\n","          loss_D_A.backward()\n","\n","          self.optimizer_D_A.step()\n","          time_DA_backward = time.time() - start_time\n","\n","          ###################################\n","\n","          ###### Discriminator B ######\n","          start_time = time.time()\n","          self.optimizer_D_B.zero_grad()\n","          pred_real, _, pred_real_cam = self.netD_B(real_B)\n","          fake_A2B = self.fake_B_buffer.push_and_pop(fake_A2B)\n","          pred_fake, _, pred_fake_cam = self.netD_B(fake_A2B.detach())\n","\n","          # Real loss\n","          loss_D_B_real = self.criterion_GAN(pred_real, self.target_real)\n","          loss_D_B_real_cam = self.criterion_GAN(pred_real_cam, \n","                                               (torch.ones_like(pred_real_cam) * REAL_LABEL).to(device))\n","          \n","          # Fake loss\n","          loss_D_B_fake = self.criterion_GAN(pred_fake, self.target_fake)\n","          loss_D_B_fake_cam = self.criterion_GAN(pred_fake_cam,\n","                                                torch.zeros_like(pred_fake_cam).to(device))\n","          \n","          # Total loss. Decide whether to add 0.5\n","          loss_D_B = coef_adv * 0.5 * (loss_D_B_real + loss_D_B_real_cam + loss_D_B_fake + loss_D_B_fake_cam)\n","          time_DB_forward = time.time() - start_time\n","\n","          start_time = time.time()\n","\n","          loss_D_B.backward()\n","          self.optimizer_D_B.step()\n","\n","          time_DB_backward = time.time() - start_time\n","          ###################################\n","\n","          step += 1\n","          if step % img_snapshot == 0:\n","            img_fake_A = 0.5 * (fake_B2A.detach().data + 1.0)\n","            img_fake_A = (to_pil(img_fake_A.data.squeeze(0).cpu()))\n","            img_fake_A.save(os.path.join(images_dir, f\"fake_A_{step}.png\"))\n","\n","            img_fake_B = 0.5 * (fake_A2B.detach().data + 1.0)\n","            img_fake_B = (to_pil(img_fake_B.data.squeeze(0).cpu()))\n","            img_fake_B.save(os.path.join(images_dir, f\"fake_B_{step}.png\"))\n","\n","          # logging\n","          if step % log_snapshot == 0:\n","            # generator\n","            self.writer.add_scalar('G/GAN_A2B', loss_GAN_A2B, global_step=step)\n","            self.writer.add_scalar('G/GAN_B2A', loss_GAN_B2A, global_step=step)\n","            self.writer.add_scalar('G/cycle_ABA', loss_cycle_ABA, global_step=step)\n","            self.writer.add_scalar('G/cycle_BAB', loss_cycle_BAB, global_step=step)\n","            self.writer.add_scalar('G/idt_A', loss_identity_A, global_step=step)\n","            self.writer.add_scalar('G/idt_B', loss_identity_B, global_step=step)\n","            self.writer.add_scalar('G/cam_A2B', loss_cam_A2B, global_step=step)\n","\n","            # discriminator\n","            self.writer.add_scalar('D/D_A', loss_D_A, global_step=step)\n","            self.writer.add_scalar('D/D_B', loss_D_B, global_step=step)\n","\n","            self.writer.add_scalar('D/D_A_real', loss_D_A_real, global_step=step)\n","            self.writer.add_scalar('D/D_A_fake', loss_D_A_fake, global_step=step)\n","\n","            self.writer.add_scalar('D/D_B_real', loss_D_B_real, global_step=step)\n","            self.writer.add_scalar('D/D_B_real_cam', loss_D_B_real_cam, global_step=step)\n","            self.writer.add_scalar('D/D_B_fake', loss_D_B_fake, global_step=step)\n","            self.writer.add_scalar('D/D_B_fake_cam', loss_D_B_fake_cam, global_step=step)\n","\n","          # validation\n","          if step % val_snapshot == 0:\n","            val_input_A = self.Tensor(1, 3, image_size, image_size)\n","            val_input_B = self.Tensor(1, 3, image_size, image_size)\n","            val_input_mask = self.Tensor(1, 1, image_size, image_size)\n","            self.netG_A2B.eval()\n","            scores_whole, scores_region = [],[]\n","            mask_scores = []\n","            for batch_val in self.dataloader_val:\n","              real_A = Variable(val_input_A.copy_(batch_val['A']))\n","              real_B = Variable(val_input_B.copy_(batch_val['B']))\n","              real_mask = Variable(val_input_mask.copy_(batch_val['mask']))\n","\n","              pred_B,_,_,_,_ = self.netG_A2B(real_A)\n","              pred_mask = mask_generator(real_A, pred_B)\n","\n","              mask_iou = iou_coef(real_mask, pred_mask, smooth=1)\n","              score_whole, score_region = evaluate(real_B, pred_B, real_mask)\n","\n","              mask_scores.append(mask_iou)\n","              scores_whole.append(score_whole)\n","              scores_region.append(score_region)\n","\n","            self.writer.add_scalar('Score/val_rmse_whole', np.mean(scores_whole), global_step=step)\n","            self.writer.add_scalar('Score/val_rmse_region', np.mean(scores_region), global_step=step)\n","            self.writer.add_scalar('Score/val_iou', np.mean(mask_scores), global_step=step)\n","\n","            self.netG_A2B.train()\n","\n","\n","\n","        # Update learning rates\n","        self.lr_scheduler_G.step()\n","        self.lr_scheduler_D_A.step()\n","        self.lr_scheduler_D_B.step()\n","\n","\n","        # Save models checkpoints\n","        if ep % model_snapshot == 0:\n","          # print(f\"Save model -- epoch: {ep}, step: {step}\")\n","          self.save_model(ep, step)\n","\n","          print('Run validation...')\n","          save_test(A_dir_inf, B_dir_inf, os.path.join(results_dir, f'msg_{ep}'))\n","          \n","\n","        epoch_cp = ep\n","\n","\n","    except Exception as e:\n","      raise e\n","    finally:\n","      print(f\"Save model -- epoch: {epoch_cp}, step: {step}\")\n","      self.save_model(epoch_cp, step)\n","\n","      print('Run validation...')\n","      save_test(A_dir_inf, B_dir_inf, os.path.join(results_dir, f'msg_{epoch_cp}'))\n","\n","  def _get_param_by_name(self, model, param_name):\n","    params = [x[1] for x in model.named_parameters() if param_name in x[0]]\n","    return {i:val for (i,val) in enumerate(params)}\n","\n","  def save_model(self, epoch_num, step):\n","    np.savetxt(os.path.join(checkpoint_dir, 'epoch_num.txt'), [epoch_num])\n","    np.savetxt(os.path.join(checkpoint_dir, 'step.txt'), [step])\n","\n","    torch.save(self.netG_A2B.state_dict(), os.path.join(checkpoint_dir, 'netG_A2B.pth'))\n","    torch.save(self.netG_B2A.state_dict(), os.path.join(checkpoint_dir, 'netG_B2A.pth'))\n","    torch.save(self.netD_A.state_dict(), os.path.join(checkpoint_dir, 'netD_A.pth'))\n","    torch.save(self.netD_B.state_dict(), os.path.join(checkpoint_dir, 'netD_B.pth'))\n","\n","    torch.save(self.optimizer_G.state_dict(), os.path.join(checkpoint_dir, 'optimizer_G.pth'))\n","    torch.save(self.optimizer_D_A.state_dict(), os.path.join(checkpoint_dir, 'optimizer_D_A.pth'))\n","    torch.save(self.optimizer_D_B.state_dict(), os.path.join(checkpoint_dir, 'optimizer_D_B.pth'))\n","\n","    torch.save(self.lr_scheduler_G.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_G.pth'))\n","    torch.save(self.lr_scheduler_D_A.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_D_A.pth'))\n","    torch.save(self.lr_scheduler_D_B.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_D_B.pth'))\n","\n","\n","  def load_model(self):\n","    epoch = int(np.loadtxt(os.path.join(checkpoint_dir, 'epoch_num.txt')))\n","    step  = int(np.loadtxt(os.path.join(checkpoint_dir, 'step.txt')))\n","    \n","    self.netG_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_A2B.pth')))\n","    self.netG_B2A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_B2A.pth')))\n","    self.netD_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netD_A.pth')))\n","    self.netD_B.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'netD_B.pth'))))\n","\n","    self.optimizer_G.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'optimizer_G.pth'))))\n","    self.optimizer_D_A.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'optimizer_D_A.pth'))))\n","    self.optimizer_D_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'optimizer_D_B.pth')))\n","\n","    self.lr_scheduler_G.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_G.pth')))\n","    self.lr_scheduler_D_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_D_A.pth')))\n","    self.lr_scheduler_D_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_D_B.pth')))\n","    print(f\"Model loaded -- {epoch}:{step}\")\n","    return epoch, step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgVInVtEI5es","colab_type":"code","outputId":"162bc93a-94ca-403e-9a40-976d5a3ccdb8","executionInfo":{"status":"ok","timestamp":1587081467999,"user_tz":-180,"elapsed":49700,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model = MaskShadowGAN().build()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["------------------- Definition of variables -------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_H8dyz8JJGE1","colab_type":"code","outputId":"fa9d1dda-d4f1-4c59-9d88-7fd40a8fa57e","executionInfo":{"status":"error","timestamp":1586450962331,"user_tz":-180,"elapsed":41719,"user":{"displayName":"Vlad Andronik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhbMGxHkjokCi9rObSClj5lf1eT5AbYqBaXBHWUw=s64","userId":"00447066830077822735"}},"colab":{"base_uri":"https://localhost:8080/","height":259,"referenced_widgets":["c6bbe9ed8d64477daadc605f6b85b671","b896c6bef06c415ba018717500b98ad5","7bb2384888b9410c992ccbc1e09faed4","4c7ae04b32804e2f9343ea7bd39a6d65","d4b9ea2d051e45118e2bb73c2064c34a","45cbe5b4a61d492b939ef51ae7ff397c","0b76a98e1bc340819ecf3b738a36db1a","b7a9603b457c45078811b2e70c3c9b05"]}},"source":["# save model\n","model.train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Resume training\n","Model loaded -- 45:60270\n","------------------- Start Training -------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6bbe9ed8d64477daadc605f6b85b671","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Run validation...\n","---- INFERENCE FINISHED. TIME : 87.5044596195221 ----\n","Run validation...\n","---- INFERENCE FINISHED. TIME : 5.464503049850464 ----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1JIndGMT7f6t","colab_type":"code","colab":{}},"source":["\"\"\"\n","function preventFromOff(){\n","  console.log(\"Click button\");\n","  document.querySelector(\"colab-connect-button\").shadowRoot.getElementById(\"connect\").click()\n","}\n","var timeout = 8 * 60 * 60 * 1000;\n","var delay = 3 * 60 * 1000;\n","var refreshId = setInterval(preventFromOff, delay);\n","setTimeout(() => {clearInterval(refreshId); console.log(\"Stopped script\");}, timeout);\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbarYuMusK8P","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7Yq4r7FsNA3","colab_type":"code","colab":{}},"source":["%tensorboard --logdir 'mask_shadow_gan/output/summary/cam_cross/summary_cam_v0.0.1/1/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFhQqJYjsXtW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}