{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mask-shadow-attention-cam-1_100.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3deacaa13aa640a38417a32a3c8493cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_db1918093f2240819284aeb2fd80cc79","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_70aee17312f140cda91736929bc2875a","IPY_MODEL_bee6680cd10a451d952e8fecaca2c8ca"]}},"db1918093f2240819284aeb2fd80cc79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70aee17312f140cda91736929bc2875a":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f654cf10647747d0bf604068c016d5cb","_dom_classes":[],"description":"  0%","_model_name":"IntProgressModel","bar_style":"","max":200,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31771332f6034daab06a6ac8b86b16a0"}},"bee6680cd10a451d952e8fecaca2c8ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d91411aad064c51996987343b039897","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/200 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea350a51e8d54b9286e22a84af9a607d"}},"f654cf10647747d0bf604068c016d5cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"31771332f6034daab06a6ac8b86b16a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d91411aad064c51996987343b039897":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ea350a51e8d54b9286e22a84af9a607d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"vUAZA8D4Q9HJ","colab_type":"text"},"source":["### CAM attention\n","  - CAM module into\n","    - G_A2B - before dilation growth\n","    - G_B2A - before dilation growth\n","    - D_A - before output layer\n","    - D_B - before output layer\n","\n","  - Coef_cam = 100\n","  - added IOU between GT and otsu\n","  - No discriminator loss division due to TTUR"]},{"cell_type":"code","metadata":{"id":"G7PsyVEfClzA","colab_type":"code","outputId":"6bc2f10a-60e5-4d47-9d13-e60cb1815a75","executionInfo":{"status":"ok","timestamp":1585240441444,"user_tz":-120,"elapsed":45725,"user":{"displayName":"Alla Andronik","photoUrl":"","userId":"03145443786364096725"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive/')\n","path_basic = 'drive/My Drive/gan_experiments'\n","os.chdir(path_basic)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2jaBlTrdCuoL","colab_type":"code","colab":{}},"source":["import torch\n","from torch.nn import init\n","from torch import nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","from torch.autograd import Variable\n","from torch.utils.tensorboard import SummaryWriter \n","\n","from skimage.filters import threshold_otsu\n","from PIL import Image\n","import cv2\n","import functools\n","import itertools\n","import numpy as np\n","import random\n","\n","import h5py\n","\n","import os\n","import sys\n","import glob\n","from tqdm import tqdm_notebook\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pickle\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eU38c1glQDp","colab_type":"text"},"source":["### Additional modules implementation"]},{"cell_type":"code","metadata":{"id":"Tbal9TpD3K2i","colab_type":"code","colab":{}},"source":["# timer utils\n","import time\n","import functools\n","def timer(f):\n","  @functools.wraps(f)\n","  def wrapper(*args, **kwargs):\n","    start_time = time.time()\n","    r = f(*args, **kwargs)\n","    duration = time.time() - start_time\n","    result = {\n","        'result':r,\n","        'time':duration\n","    }\n","    return result\n","  return wrapper"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8aJdPS2HI3h","colab_type":"code","colab":{}},"source":["class CAMAttention(nn.Module):\n","  def __init__(self, input_nc):\n","    super(CAMAttention, self).__init__()\n","    self.gap_fc = nn.utils.spectral_norm(nn.Linear(input_nc, 1, bias=False))\n","    self.gmp_fc = nn.utils.spectral_norm(nn.Linear(input_nc, 1, bias=False))\n","\n","    self.conv_1x1 = nn.utils.spectral_norm(nn.Conv2d(input_nc * 2, input_nc, 1))\n","    self.relu = nn.ReLU(True)\n","\n","\n","  def forward(self, x):\n","    # global avg. pool.\n","    gap = F.adaptive_avg_pool2d(x, 1)\n","    gap = gap.view(gap.shape[0], -1)\n","    gap_fc = self.gap_fc(gap)\n","    gap_weights = next(self.gap_fc.parameters()).unsqueeze(2).unsqueeze(3)\n","    gap_output = x * gap_weights\n","\n","    # global max. pool.\n","    gmp = F.adaptive_max_pool2d(x, 1)\n","    gmp = gmp.view(gmp.shape[0], -1)\n","    gmp_fc = self.gmp_fc(gmp)\n","    gmp_weights = next(self.gmp_fc.parameters()).unsqueeze(2).unsqueeze(3)\n","    gmp_output = x * gmp_weights\n","\n","    cam_loss = torch.cat([gap_fc, gmp_fc], axis=1)\n","    gap_gmp_output = torch.cat([gap_output, gmp_output], axis=1)\n","\n","    \n","    output = self.relu(self.conv_1x1(gap_gmp_output))\n","    attention = torch.sum(output, axis=1, keepdims=True)\n","\n","    return output, attention, cam_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeEmGUYFIUi8","colab_type":"code","colab":{}},"source":["test = torch.rand(1,3,32,32)\n","model = CAMAttention(3)\n","out = model(test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llViHAJ2EPEn","colab_type":"text"},"source":["### Blocks"]},{"cell_type":"code","metadata":{"id":"hM7rtwdjEPAO","colab_type":"code","colab":{}},"source":["class ResidualBlock(nn.Module):\n","    \"\"\"\n","      Parameters:\n","        dilation_factor -- dilated convolution hyperparam\n","    \"\"\"\n","    def __init__(self, in_features, dilation_factor, r=4):\n","        super(ResidualBlock, self).__init__()\n","        k = 3\n","        d = dilation_factor\n","        pad = int(((k - 1) * (d - 1) + k - 1) / 2)\n","\n","        conv_block = [  nn.ReflectionPad2d(pad),\n","                        nn.utils.spectral_norm(nn.Conv2d(in_features, in_features, 3, dilation=d)),\n","                        nn.InstanceNorm2d(in_features),\n","                        nn.ReLU(inplace=True),\n","                      \n","                        nn.ReflectionPad2d(pad),\n","                        nn.utils.spectral_norm(nn.Conv2d(in_features, in_features, 3, dilation=d)),\n","                        nn.InstanceNorm2d(in_features)\n","                      ]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class Generator_S2F(nn.Module):\n","    \"\"\"\n","      attn_idx -- index of the bottleneck block onwards to place attention block\n","    \"\"\"\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=9,\n","                 dilation_factors=[1,1,1,2,4,8,16,1,1],\n","                 attn_idx=4):\n","        super(Generator_S2F, self).__init__()\n","        assert len(dilation_factors) == n_residual_blocks\n","\n","        downsample = []\n","        bottleneck_1 = []  # modules before attention\n","        bottleneck_2 = []  # modules after attention\n","        upsample = []\n","\n","        # Initial convolution block\n","        downsample += [   nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(input_nc, 64, 7)),\n","                    nn.InstanceNorm2d(64),\n","                    nn.ReLU(inplace=True) ]\n","\n","        # Downsampling\n","        in_features = 64\n","        out_features = in_features*2\n","        for _ in range(2):\n","            downsample += [  nn.utils.spectral_norm(nn.Conv2d(in_features, out_features, 3, stride=2, padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features*2\n","\n","        # Residual blocks\n","        # Paste attn module before dilation growth\n","        for i in range(attn_idx):\n","          d = dilation_factors[i]\n","          bottleneck_1 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        self.cam_attn = CAMAttention(in_features)\n","        for i in range(attn_idx, n_residual_blocks):\n","          d = dilation_factors[i]\n","          bottleneck_2 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        # Upsampling\n","        out_features = in_features//2\n","        for _ in range(2):\n","            upsample += [  nn.utils.spectral_norm(nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features//2\n","\n","        # Output layer\n","        upsample += [  nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(64, output_nc, 7)) ]\n","                    #nn.Tanh() ]\n","\n","        self.downsample = nn.Sequential(*downsample)\n","        self.bottleneck_1 = nn.Sequential(*bottleneck_1)\n","        self.bottleneck_2 = nn.Sequential(*bottleneck_2)\n","        self.upsample = nn.Sequential(*upsample)\n","\n","    def forward(self, x):\n","      downsampled = self.downsample(x)\n","\n","      bottleneck_1 = self.bottleneck_1(downsampled)\n","      cam_output, cam_attn, cam_loss = self.cam_attn(bottleneck_1)\n","      bottleneck_2 = self.bottleneck_2(cam_output)\n","\n","      output = self.upsample(bottleneck_2)\n","      output = (output + x).tanh() #(min=-1, max=1) #just learn a residual\n","      return output, cam_attn, cam_loss\n","\n","\n","class Generator_F2S(nn.Module):\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=9,\n","                 dilation_factors=[1,1,1,2,4,8,16,1,1],\n","                 attn_idx=-1):\n","        super(Generator_F2S, self).__init__()\n","        assert len(dilation_factors) == n_residual_blocks\n","        self.attn_idx = attn_idx\n","        downsample = []\n","        if self.attn_idx > 0:\n","          bottleneck_1 = []  # modules before attention\n","          bottleneck_2 = []  # modules after attention\n","        else:\n","          bottleneck = []\n","\n","        upsample = []\n","\n","        # Initial convolution block\n","        downsample += [   nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(input_nc+1, 64, 7)), # + mask\n","                    nn.InstanceNorm2d(64),\n","                    nn.ReLU(inplace=True) ]\n","\n","        # Downsampling\n","        in_features = 64\n","        out_features = in_features*2\n","        for _ in range(2):\n","            downsample += [  nn.utils.spectral_norm(nn.Conv2d(in_features, out_features, 3, stride=2, padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features*2\n","\n","        # Residual blocks\n","        # Paste attn module before dilation growth\n","        if self.attn_idx <= 0:\n","          for i in range(n_residual_blocks):\n","            d = dilation_factors[i]\n","            bottleneck += [ResidualBlock(in_features,dilation_factor=d)]\n","        else:\n","          for i in range(attn_idx):\n","            d = dilation_factors[i]\n","            bottleneck_1 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","          self.cam_attn = CAMAttention(in_features)\n","          for i in range(attn_idx, n_residual_blocks):\n","            d = dilation_factors[i]\n","            bottleneck_2 += [ResidualBlock(in_features,dilation_factor=d)]\n","\n","        # Upsampling\n","        out_features = in_features//2\n","        for _ in range(2):\n","            upsample += [  nn.utils.spectral_norm(nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1)),\n","                        nn.InstanceNorm2d(out_features),\n","                        nn.ReLU(inplace=True) ]\n","            in_features = out_features\n","            out_features = in_features//2\n","\n","        # Output layer\n","        upsample += [  nn.ReflectionPad2d(3),\n","                    nn.utils.spectral_norm(nn.Conv2d(64, output_nc, 7)) ]\n","                    #nn.Tanh() ]\n","\n","        self.downsample = nn.Sequential(*downsample)\n","        self.upsample = nn.Sequential(*upsample)\n","\n","        if self.attn_idx > 0:\n","          self.bottleneck_1 = nn.Sequential(*bottleneck_1)\n","          self.bottleneck_2 = nn.Sequential(*bottleneck_2)\n","        else:\n","          model = downsample + bottleneck + upsample\n","          self.model = nn.Sequential(*model)\n","\n","\n","    def forward(self, x, mask):\n","      if self.attn_idx <= 0:\n","        return (self.model(torch.cat((x, mask), 1)) + x).tanh() #(min=-1, max=1) #just learn a residual\n","\n","      downsampled = self.downsample(torch.cat((x, mask), 1))\n","\n","      bottleneck_1 = self.bottleneck_1(downsampled)\n","      cam_output, cam_attn, cam_loss = self.cam_attn(bottleneck_1)\n","      bottleneck_2 = self.bottleneck_2(cam_output)\n","\n","      output = self.upsample(bottleneck_2)\n","      output = (output + x).tanh()\n","      return output, cam_attn, cam_loss\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_nc, attn=True):\n","        super(Discriminator, self).__init__()\n","\n","        # A bunch of convolutions one after another\n","        self.attn = attn\n","        model = []\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(input_nc, 64, 4, stride=2, padding=1)),\n","                 nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),\n","                  nn.InstanceNorm2d(128),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),\n","                  nn.InstanceNorm2d(256),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        model += [nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, padding=1)),\n","                  nn.InstanceNorm2d(512),\n","                  nn.LeakyReLU(0.2, inplace=True) ]\n","\n","        # CAM layers\n","        if self.attn:\n","          self.cam_attn = CAMAttention(512)\n","          self.conv_output = nn.utils.spectral_norm(nn.Conv2d(512, 1, 4, padding=1))\n","        else:\n","          model += [nn.utils.spectral_norm(nn.Conv2d(512, 1, 4, padding=1))]\n","\n","        self.model = nn.Sequential(*model)\n","\n","\n","    def forward(self, x):\n","      x = self.model(x)\n","      if not self.attn:\n","        # Average pooling and flatten\n","        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1) #global avg pool\n","\n","      cam_output, cam_attn, cam_loss = self.cam_attn(x)\n","      output = self.conv_output(cam_output)\n","      output = F.avg_pool2d(output, output.size()[2:]).view(output.size()[0], -1) #global avg pool\n","      return output, cam_attn, cam_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-b93raDXeCd","colab_type":"code","colab":{}},"source":["test = torch.rand(1,3,256,256)\n","gab = Generator_S2F(3,3,attn_idx=4)\n","gba_attn = Generator_S2F(3,3,attn_idx=4)\n","gba_noattn = Generator_S2F(3,3,attn_idx=-1)\n","d_attn = Discriminator(3, attn=True)\n","d_noattn = Discriminator(3, attn=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Jof0vreXd07","colab_type":"code","colab":{}},"source":["gab_out, gab_attn, gab_cam = gab(test)\n","gba_attn_out, gba_attn_attn, gba_attn_cam = gba_attn(test)\n","gba_noattn_out = gba_noattn(test)\n","\n","d_attn_out, d_attn_attn, d_attn_cam = d_attn(test)\n","d_noattn_out = d_noattn(test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4o0Q56KWb1E","colab_type":"code","outputId":"724e1f36-6226-4a6b-9638-5b1b18951752","executionInfo":{"status":"ok","timestamp":1584915955784,"user_tz":-120,"elapsed":1268,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["d_noattn_out"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.2523]], grad_fn=<ViewBackward>)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"L3lYJ6CkYFXy","colab_type":"code","colab":{}},"source":["def cam(x, size = 256):\n","    x = x - np.min(x)\n","    cam_img = x / np.max(x)\n","    cam_img = np.uint8(255 * cam_img)\n","    cam_img = cv2.resize(cam_img, (size, size))\n","    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n","    return cam_img / 255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hv8Uc0N3YFOx","colab_type":"code","colab":{}},"source":["gab_attn = cam(gab_attn.detach().numpy().squeeze())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tc-v0TIPGM7u","colab_type":"code","colab":{}},"source":["plt.imshow(gab_attn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YJi52j7GIi1","colab_type":"text"},"source":["### Buffers"]},{"cell_type":"code","metadata":{"id":"Eek1JNenFvmu","colab_type":"code","colab":{}},"source":["class ReplayBuffer():\n","    def __init__(self, max_size=50):\n","        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0,1) > 0.5:\n","                    i = random.randint(0, self.max_size-1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))\n","\n","class QueueMask():\n","    def __init__(self, length):\n","        self.max_length = length\n","        self.queue = []\n","\n","    def insert(self, mask):\n","        if self.queue.__len__() >= self.max_length:\n","            self.queue.pop(0)\n","\n","        self.queue.append(mask)\n","\n","    def rand_item(self):\n","        assert self.queue.__len__() > 0, 'Error! Empty queue!'\n","        return self.queue[np.random.randint(0, self.queue.__len__())]\n","\n","    def last_item(self):\n","        assert self.queue.__len__() > 0, 'Error! Empty queue!'\n","        return self.queue[self.queue.__len__()-1]\n","        \n","\n","class ImageDatasetHDF5(Dataset):\n","    def __init__(self, path_hdf5, transforms_=None):\n","        self.transform = transforms.Compose(transforms_)\n","        self.files = h5py.File(path_hdf5, \"r\")\n","        self.domain_A = 'domain_A'\n","        self.domain_B = 'domain_B'\n","\n","        self.length_A = self.files[self.domain_A].shape[0]\n","        self.length_B = self.files[self.domain_B].shape[0]\n","\n","    @timer\n","    def __getitem__(self, index):\n","        item_A = self.transform(self.files[self.domain_A][index % self.length_A])\n","        item_B = self.transform(self.files[self.domain_B][random.randint(0, self.length_B - 1)])\n","        return {'A': item_A, 'B': item_B}\n","\n","    def __len__(self):\n","        return max(self.length_A, self.length_B)\n","\n","\n","class ImageDatasetValidation(Dataset):\n","    def __init__(self, root, paired=False, transforms_=None, mode='val', mask=False):\n","        self.transform = transforms.Compose(transforms_)\n","        self.paired = paired\n","        self.files_A = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_A', '*.png')))\n","        self.files_B = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_C', '*.png')))\n","        self.mask = mask\n","        self.totensor = transforms.ToTensor()\n","        if self.mask:\n","          self.transform_mask = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","                                                    transforms.ToTensor()])\n","          self.files_mask = sorted(glob.glob(os.path.join(root, f'{mode}/{mode}_B', '*.png')))\n","\n","    def __getitem__(self, index):\n","        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n","        \n","        if self.paired:\n","          item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n","          if self.mask:\n","            item_mask = self.transform_mask(Image.open(self.files_mask[index % len(self.files_mask)]))\n","            return {'A': item_A, 'B': item_B, 'mask':item_mask}  \n","          return {'A': item_A, 'B': item_B}  \n","        item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n","        return {'A': item_A, 'B': item_B}\n","\n","    def __len__(self):\n","        return max(len(self.files_A), len(self.files_B))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bcS40lq_tcV","colab_type":"code","colab":{}},"source":["# V0.1.0 num params\n","V010_G = 45.512\n","V010_D = 11.058\n","V010_S = 56.57"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWucd1lR_ZiX","colab_type":"code","outputId":"6553c221-0daf-4d96-86c0-f5283f73699a","executionInfo":{"status":"ok","timestamp":1585179182381,"user_tz":-120,"elapsed":7016,"user":{"displayName":"Vlad Andronik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhbMGxHkjokCi9rObSClj5lf1eT5AbYqBaXBHWUw=s64","userId":"00447066830077822735"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# summary of the models\n","def num_params(model):\n","  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","  params = sum([np.prod(p.size()) for p in model_parameters])\n","  return params\n","\n","dL = Discriminator(3, attn=True)\n","g = Generator_S2F(3,3)\n","\n","V011_D = round(num_params(dL) * 2 / 1e6, 3) * 2\n","V011_G = round(num_params(g) * 2 / 1e6, 3) * 2\n","V011_S = round(V011_D + V011_G, 3)\n","\n","print('---- Summary models ----')\n","print(\"Number of parameters (in millions):\")\n","print(\"{:10}{:10}{:20}\".format(\"D\", 'G', \"Overall\"))\n","print(\"{:10}{:10}{:20}\".format(str(V011_D), str(V011_G), str(V011_S)))\n","\n","print(f\"GAIN PARAMS: {round((V011_S / V010_S - 1) * 100, 3)}%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["---- Summary models ----\n","Number of parameters (in millions):\n","D         G         Overall             \n","13.162    46.04     59.202              \n","GAIN PARAMS: 4.653%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ih5jjEJIGPwI","colab_type":"text"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"KsAG2hLaFyt5","colab_type":"code","colab":{}},"source":["to_pil = transforms.ToPILImage()\n","to_gray = transforms.Grayscale(num_output_channels=1)\n","\n","\n","def mask_generator(shadow, shadow_free):\n","\tim_f = to_gray(to_pil(((shadow_free.data.squeeze(0) + 1.0) * 0.5).cpu()))\n","\tim_s = to_gray(to_pil(((shadow.data.squeeze(0) + 1.0) * 0.5).cpu()))\n","\n","\tdiff = (np.asarray(im_f, dtype='float32')- np.asarray(im_s, dtype='float32')) # difference between shadow image and shadow_free image\n","\tL = threshold_otsu(diff)\n","\tmask = torch.tensor((np.float32(diff >= L)-0.5)/0.5).unsqueeze(0).unsqueeze(0).cuda() #-1.0:non-shadow, 1.0:shadow\n","\tmask.requires_grad = False\n","\treturn mask\n","\n","\n","\n","def tensor2image(tensor):\n","    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n","    if image.shape[0] == 1:\n","        image = np.tile(image, (3,1,1))\n","    return image.astype(np.uint8)\n","\n","def cam(x, size = 256):\n","    x = x - np.min(x)\n","    cam_img = x / np.max(x)\n","    cam_img = np.uint8(255 * cam_img)\n","    cam_img = cv2.resize(cam_img, (size, size))\n","    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n","    return cam_img\n","\n","\n","class LambdaLR():\n","    def __init__(self, n_epochs, offset, decay_start_epoch):\n","        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n","        self.n_epochs = n_epochs\n","        self.offset = offset\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch):\n","        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n","\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRZ9EoJlwsnM","colab_type":"text"},"source":["### Evaluators"]},{"cell_type":"code","metadata":{"id":"TkuvuH0W4x7O","colab_type":"code","colab":{}},"source":["def read_paths(path):\n","  with open(path, 'rb') as f:\n","    paths = pickle.load(f)\n","  return paths\n","\n","def mkdir(path):\n","  try:\n","    os.mkdir(path)\n","  except FileExistsError as e:\n","    pass\n","\n","def save_test(A_path, B_path, save_path):\n","  start_time = time.time()\n","  A_paths = read_paths(A_path)\n","  B_paths = read_paths(B_path)\n","  \n","  assert len(A_paths) == len(B_paths)\n","  # read and preprocess the test images\n","  os.makedirs(save_path, exist_ok=True)\n","\n","  mkdir(os.path.join(save_path, 'A_B'))\n","  mkdir(os.path.join(save_path, 'B_A'))\n","\n","  mkdir(os.path.join(save_path, 'masks'))\n","  mkdir(os.path.join(save_path, 'attn_A_B'))\n","  mkdir(os.path.join(save_path, 'attn_B_A'))\n","\n","  # load the model\n","  netG_A2B = Generator_S2F(input_nc, output_nc, attn_idx=attn_idx_GAB).to(device)\n","  netG_B2A = Generator_F2S(input_nc, output_nc, attn_idx=attn_idx_GBA).to(device)\n","\n","\n","  # load latest checkpoint\n","  netG_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_A2B.pth')))\n","  netG_B2A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_B2A.pth')))\n","\n","  # turn the validation mode\n","  netG_A2B.eval()\n","  netG_B2A.eval()\n","\n","  # input tensors\n","  Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n","  input_A = Tensor(batch_size, input_nc, image_size, image_size, 3)\n","  input_B = Tensor(batch_size, output_nc, image_size, image_size, 3)\n","\n","  # input transformations\n","  img_transforms = transforms.Compose([\n","                                       transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n","  ])\n","  to_pil = transforms.ToPILImage()\n","\n","  image_queue = QueueMask(length=mask_queue_size)\n","  for i,(path_A, path_B) in enumerate(zip(A_paths, B_paths)):\n","    image_A = Image.open(path_A).convert(\"RGB\")\n","    image_B = Image.open(path_B).convert(\"RGB\")\n","\n","    im_A = (img_transforms(image_A).unsqueeze(0)).to(device)\n","    im_B = (img_transforms(image_B).unsqueeze(0)).to(device)\n","\n","  \n","\n","    # generate A -> B\n","    A_B, attn_A_B, _ = netG_A2B(im_A)\n","    w,h = image_A.size\n","\n","    current_mask = mask_generator(A_B, im_A)\n","    image_queue.insert(current_mask)\n","    A_B = .5 * (A_B + 1)\n","    A_B = np.array((to_pil(A_B.data.squeeze(0).cpu())))\n","    attn_A_B = cam(attn_A_B.data.squeeze().cpu().numpy(), size=w)\n","    Image.fromarray(A_B).save(os.path.join(save_path, 'A_B', 'A_B_{}.jpg'.format(i)))\n","    Image.fromarray(attn_A_B).save(os.path.join(save_path, 'attn_A_B', 'A_B_{}.jpg'.format(i)))\n","\n","    # generate B -> A\n","    mask = image_queue.rand_item()\n","    B_A,attn_B_A, _ = netG_B2A(im_B, mask)\n","    w,h = image_B.size\n","\n","    B_A = .5 * (B_A + 1)\n","    B_A = np.array((to_pil(B_A.data.squeeze(0).cpu())))\n","    attn_B_A = cam(attn_B_A.data.squeeze().cpu().numpy(), size=w)\n","    Image.fromarray(B_A).save(os.path.join(save_path, 'B_A', 'B_A_{}.jpg'.format(i)))\n","    Image.fromarray(attn_B_A).save(os.path.join(save_path, 'attn_B_A', 'B_A_{}.jpg'.format(i)))\n","\n","    mask_cpu = np.array((to_pil(.5 * (current_mask.data + 1).squeeze(0).cpu())))\n","    Image.fromarray(mask_cpu).save(os.path.join(save_path, 'masks', 'mask_{}.jpg'.format(i)))\n","\n","  print(\"---- Inference finished. Time : {} ----\".format(time.time() - start_time).upper())\n","  return image_queue\n","\n","\n","def save_images(image_paths, save_path, domain):\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    pass\n","\n","  for i in range(len(image_paths)):\n","    img = plt.imread(image_paths[i])\n","    plt.imsave(os.path.join(save_path,'{}_{}.jpg'.format(domain, i)), img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7c7noBkq0TmC","colab_type":"code","colab":{}},"source":["from skimage import color, io\n","def evaluate(image_true, image_pred, mask):\n","    mask = torch.where(mask.expand(-1, 3, -1, -1))\n","    image_true_shreg = image_true[mask].squeeze().detach().cpu().numpy()\n","    image_pred_shreg = image_pred[mask].squeeze().detach().cpu().numpy()\n","    image_true_shreg = 127.5 * (image_true_shreg + 1)\n","    image_pred_shreg = 127.5 * (image_pred_shreg + 1)\n","\n","    image_true = convert_image_array(image_true)\n","    image_pred = convert_image_array(image_pred)\n","\n","    image_true = color.rgb2lab(image_true)\n","    image_pred = color.rgb2lab(image_pred)\n","\n","\n","    rmse_whole = np.sqrt(np.square(image_true - image_pred))\n","    rmse_shreg = np.sqrt(np.square(image_true_shreg - image_pred_shreg))\n","    return np.mean(rmse_whole), np.mean(rmse_shreg)\n","\n","\n","def convert_image_array(image):\n","  image = 127.5 * (image.squeeze(0) + 1)\n","  image = image.permute(1,2,0)\n","  \n","  return image.data.cpu().numpy().astype(np.uint8)\n","\n","\n","def decode(image):\n","  image = 127.5 * (image + 1)\n","  return image.data.cpu()\n","\n","def iou_coef(y_true, y_pred, smooth=1):\n","  y_true = y_true.data.cpu()\n","  y_pred=  decode(y_pred)\n","  intersection = torch.sum(y_true * y_pred, axis=[1,2,3])\n","  union = torch.sum(y_true, axis=[1,2,3]) + torch.sum(y_pred, axis=[1,2,3]) - intersection\n","\n","  return torch.mean((intersection + smooth) / (union + smooth), axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewGd-NfG8m3N","colab_type":"code","outputId":"b7d77b59-1309-4fe9-9718-c4f663f060d9","executionInfo":{"status":"ok","timestamp":1585179182390,"user_tz":-120,"elapsed":6926,"user":{"displayName":"Vlad Andronik","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhbMGxHkjokCi9rObSClj5lf1eT5AbYqBaXBHWUw=s64","userId":"00447066830077822735"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["im1, im2 = torch.rand(1,3,32,32), torch.rand(1,3,32,32)\n","mask = torch.randint(0,2,size=(1,1,32,32))\n","evaluate(im1, im2, mask)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(21.46908819098392, 41.480007)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"hXJjtqf5IL_o","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"sR8ZGJ5Z5_PK","colab_type":"code","colab":{}},"source":["# _ = [os.remove(os.path.join(summary_dir, path)) for path in os.listdir(summary_dir)]\n","# _ = [os.remove(os.path.join(images_dir, path)) for path in os.listdir(images_dir)]\n","# _ = [os.remove(os.path.join(checkpoint_dir, path)) for path in os.listdir(checkpoint_dir)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOpXykMJIK_a","colab_type":"code","colab":{}},"source":["checkpoint_dir = 'mask_shadow_gan/output/checkpoints/checkpoints_cam_v0.0.1/3/'\n","images_dir = 'mask_shadow_gan/output/images/images_cam_v0.0.1/3/'\n","summary_dir = 'mask_shadow_gan/output/summary/summary_cam_v0.0.1/3/'\n","\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","os.makedirs(images_dir, exist_ok=True)\n","os.makedirs(summary_dir, exist_ok=True)\n","\n","root_dir = 'data/ISTD_Dataset/'\n","hdf5_file = 'data/ISTD_Dataset/hdf5_dataset/data_istd.h5'\n","\n","# validation pathes\n","A_dir_inf = 'mask_shadow_gan/output/results/test_set_meta/test_paths/ISTD/shadow_path.pickle'\n","B_dir_inf = 'mask_shadow_gan/output/results/test_set_meta/test_paths/ISTD/free_path.pickle'\n","results_dir = 'mask_shadow_gan/output/results/cam_v.0.0.1/3'\n","\n","\n","\n","load_model = False\n","batch_size=1\n","image_size=256\n","ngf=64\n","ndf=64\n","\n","lambda1=10\n","lambda2=10\n","identity_lambda = 0.5\n","learning_rate=2e-4\n","lr_D = 4e-4  # TTUR\n","lr_G = 1e-4  # TTUR\n","\n","beta1=.5\n","mask_queue_size=50\n","slope=0.2\n","stddev=0.02\n","\n","input_nc=3\n","output_nc=3\n","\n","n_res_blocks=9\n","dilation_A2B = [1,1,1,1,1,2,2,4,8]\n","dilation_B2A = [1,1,1,1,1,2,2,4,8]\n","REAL_LABEL=0.9  # label smoothing\n","\n","device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","n_epochs=200\n","decay_epoch=100\n","epoch=0\n","img_snapshot=500\n","model_snapshot=5\n","log_snapshot=10\n","val_snapshot=100\n","\n","coef_identity = 5\n","coef_cycle = 10\n","coef_adv = 1\n","coef_cam = 100\n","\n","attn_idx_GAB = 5\n","attn_idx_GBA = 5\n","attn_DA = True\n","attn_DB = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsP_efe1yU6u","colab_type":"code","colab":{}},"source":["class MaskShadowGAN(object):\n","  def __init__(self):\n","    pass\n","\n","  def build(self):\n","    ###### Definition of variables ######\n","    # Networks\n","    print(f\"------------------- Definition of variables -------------------\")\n","    \n","    self.netG_A2B = Generator_S2F(input_nc, output_nc, \n","                                  n_residual_blocks=n_res_blocks,\n","                                  dilation_factors=dilation_A2B, attn_idx=attn_idx_GAB)  # shadow to shadow_free\n","    self.netG_B2A = Generator_F2S(output_nc, input_nc,\n","                                  n_residual_blocks=n_res_blocks,\n","                                  dilation_factors=dilation_B2A, attn_idx=attn_idx_GBA)  # shadow_free to shadow\n","    self.netD_A = Discriminator(input_nc, attn=attn_DA)\n","    self.netD_B = Discriminator(output_nc, attn=attn_DB)\n","\n","\n","    self.netG_A2B.cuda()\n","    self.netG_B2A.cuda()\n","    self.netD_A.cuda()\n","    self.netD_B.cuda()\n","\n","    self.netG_A2B.apply(weights_init_normal)\n","    self.netG_B2A.apply(weights_init_normal)\n","    self.netD_A.apply(weights_init_normal)\n","    self.netD_B.apply(weights_init_normal)\n","\n","    # Lossess\n","    self.criterion_GAN = torch.nn.MSELoss().to(device)  # lsgan\n","    # criterion_GAN = torch.nn.BCEWithLogitsLoss() #vanilla\n","    self.criterion_cycle = torch.nn.L1Loss().to(device)\n","    self.criterion_identity = torch.nn.L1Loss().to(device)\n","    self.criterion_cam = nn.BCEWithLogitsLoss().to(device)\n","\n","    # Optimizers & LR schedulers\n","    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A2B.parameters(), self.netG_B2A.parameters()),\n","                    lr=lr_G, betas=(0.5, 0.999))\n","    self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=lr_D, betas=(0.5, 0.999))\n","    self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=lr_D, betas=(0.5, 0.999))\n","\n","    self.lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(self.optimizer_G,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    self.lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D_A,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    self.lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D_B,\n","                              lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    \n","    self.Tensor = torch.cuda.FloatTensor\n","    self.input_A = self.Tensor(batch_size, input_nc, image_size, image_size)\n","    self.input_B = self.Tensor(batch_size, output_nc, image_size, image_size)\n","    self.target_real = Variable(self.Tensor(batch_size).fill_(REAL_LABEL), requires_grad=False)\n","    self.target_fake = Variable(self.Tensor(batch_size).fill_(0.0), requires_grad=False)\n","    self.mask_non_shadow = Variable(self.Tensor(batch_size, 1, image_size, image_size).fill_(-1.0), requires_grad=False) #-1.0 non-shadow\n","\n","    self.fake_A_buffer = ReplayBuffer()\n","    self.fake_B_buffer = ReplayBuffer()\n","\n","    # Dataset loader\n","    self.transforms_ = [\n","            # transforms.ToPILImage(),\n","            transforms.Resize(int(image_size * 1.12), Image.BICUBIC),\n","            transforms.RandomCrop(image_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","\n","    transforms_test = [\n","      transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n","      transforms.ToTensor(),\n","      transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n","    ]\n","\n","    # self.dataloader = DataLoader(ImageDatasetHDF5(hdf5_file, transforms_=self.transforms_),\n","    #                                           batch_size=batch_size, shuffle=True, num_workers=1)\n","    \n","    self.dataloader_train = DataLoader(ImageDatasetValidation(root_dir, paired=False, \n","                                                              transforms_=self.transforms_, \n","                                                              mode='train'), \n","                                    batch_size=1, shuffle=False, num_workers=os.cpu_count())\n","    self.dataloader_val = DataLoader(ImageDatasetValidation(root_dir, paired=True, \n","                                                            transforms_=transforms_test, \n","                                                            mode='val', mask=True ), \n","                                    batch_size=1, shuffle=False, num_workers=os.cpu_count())\n","    \n","\n","    \"\"\"Summary writing to tensorboard\"\"\"\n","    self.writer = SummaryWriter(log_dir=summary_dir)\n","    return self\n","    \n","\n","  def train(self):\n","    step = 0\n","    epoch = 0\n","    if load_model:\n","      print(\"Resume training\")\n","      epoch, step = self.load_model()\n","      # epoch+=1\n","\n","    to_pil = transforms.ToPILImage()\n","    mask_queue =  QueueMask(self.dataloader_train.__len__()/4)\n","\n","    ###### Training ######\n","    print(f\"------------------- Start Training -------------------\")\n","    epoch_cp = epoch\n","    try:\n","      for ep in tqdm_notebook(range(epoch, n_epochs), total=n_epochs-epoch):\n","        for i, batch in enumerate(self.dataloader_train):\n","          # Set model input\n","          # batch = _batch['result']\n","          # time_batch = _batch['time']\n","          real_A = Variable(self.input_A.copy_(batch['A']))\n","          real_B = Variable(self.input_B.copy_(batch['B']))\n","\n","          ###### Generators A2B and B2A ######\n","          start_time = time.time()\n","          self.optimizer_G.zero_grad()\n","          \n","          # forward\n","          fake_A2B, _, fake_A2B_cam = self.netG_A2B(real_A)\n","          mask_queue.insert(mask_generator(real_A, fake_A2B))\n","          fake_B2A, _, fake_B2A_cam = self.netG_B2A(real_B, mask_queue.rand_item())\n","\n","          # cycle\n","          fake_A2B2A, _, _ = self.netG_B2A(fake_A2B, mask_queue.last_item())\n","          fake_B2A2B, _, _ = self.netG_A2B(fake_B2A)\n","\n","          # identity\n","          fake_B2B, _, fake_B2B_cam = self.netG_A2B(real_B)  # do not remove shadow if nothing to remove\n","          fake_A2A, _, fake_A2A_cam = self.netG_B2A(real_A, self.mask_non_shadow)  # do not generate shadow if nothing to generate\n","\n","          # disc preds\n","          pred_fake_A2B, _, pred_fake_A2B_cam = self.netD_B(fake_A2B)\n","          pred_fake_B2A, _, pred_fake_B2A_cam = self.netD_A(fake_B2A)\n","\n","          # losses\n","          # adversarial\n","          loss_GAN_A2B = self.criterion_GAN(pred_fake_A2B, self.target_real)\n","          loss_GAN_cam_A2B = self.criterion_GAN(pred_fake_A2B_cam, \n","                                                (torch.ones_like(pred_fake_A2B_cam) * REAL_LABEL).to(device))\n","\n","          loss_GAN_B2A = self.criterion_GAN(pred_fake_B2A, self.target_real)\n","          loss_GAN_cam_B2A = self.criterion_GAN(pred_fake_B2A_cam, \n","                                                (torch.ones_like(pred_fake_B2A_cam) * REAL_LABEL).to(device))\n","\n","\n","          # cam loss as binary classification problem. \n","          # assign higher probs to images from current domain than to the other ones.\n","          # cam loss for discriminator is the same as its adv. counterpart\n","          loss_cam_A2B = self.criterion_cam(fake_A2B_cam, torch.ones_like(fake_A2B_cam).to(device)) + \\\n","            self.criterion_cam(fake_B2B_cam, torch.zeros_like(fake_B2B_cam).to(device))\n","          loss_cam_B2A = self.criterion_cam(fake_B2A_cam, torch.ones_like(fake_B2A_cam).to(device)) + \\\n","            self.criterion_cam(fake_A2A_cam, torch.zeros_like(fake_A2A_cam).to(device))\n","          \n","          # identity loss\n","          loss_identity_A = self.criterion_identity(fake_A2A, real_A)\n","          loss_identity_B = self.criterion_identity(fake_B2B, real_B)\n","\n","          # cycle loss\n","          loss_cycle_BAB = self.criterion_cycle(real_B, fake_B2A2B)\n","          loss_cycle_ABA = self.criterion_cycle(real_A, fake_A2B2A)\n","\n","          loss_G = coef_adv * (loss_GAN_A2B + loss_GAN_cam_A2B + loss_GAN_B2A + loss_GAN_cam_B2A) + \\\n","            coef_cam * (loss_cam_A2B + loss_cam_B2A) + coef_cycle * (loss_cycle_ABA + loss_cycle_BAB) + \\\n","            coef_identity * (loss_identity_A + loss_identity_B)\n","\n","          start_time = time.time()\n","          loss_G.backward()\n","          self.optimizer_G.step()\n","          time_G_backward = time.time() - start_time\n","\n","          ###################################\n","\n","          ###### Discriminator A ######\n","          start_time = time.time()\n","          self.optimizer_D_A.zero_grad()\n","\n","          pred_real, _, pred_real_cam = self.netD_A(real_A)\n","          fake_B2A = self.fake_A_buffer.push_and_pop(fake_B2A)\n","          pred_fake, _, pred_fake_cam = self.netD_A(fake_B2A.detach())\n","\n","          # Real loss\n","          loss_D_A_real = self.criterion_GAN(pred_real, self.target_real)\n","          loss_D_A_real_cam = self.criterion_GAN(pred_real_cam, \n","                                               (torch.ones_like(pred_real_cam) * REAL_LABEL).to(device))\n","          \n","          # Fake loss\n","          loss_D_A_fake = self.criterion_GAN(pred_fake, self.target_fake)\n","          loss_D_A_fake_cam = self.criterion_GAN(pred_fake_cam,\n","                                                torch.zeros_like(pred_fake_cam).to(device))\n","          \n","          # Total loss. Decide whether to scale by 0.5(0.25): Better to add for consistency with prev. results\n","          loss_D_A = coef_adv * 0.5 * (loss_D_A_real + loss_D_A_real_cam + loss_D_A_fake + loss_D_A_fake_cam)\n","\n","          time_DA_forward = time.time() - start_time\n","          \n","          start_time = time.time()\n","          loss_D_A.backward()\n","\n","          self.optimizer_D_A.step()\n","          time_DA_backward = time.time() - start_time\n","\n","          ###################################\n","\n","          ###### Discriminator B ######\n","          start_time = time.time()\n","          self.optimizer_D_B.zero_grad()\n","          pred_real, _, pred_real_cam = self.netD_B(real_B)\n","          fake_A2B = self.fake_B_buffer.push_and_pop(fake_A2B)\n","          pred_fake, _, pred_fake_cam = self.netD_B(fake_A2B.detach())\n","\n","          # Real loss\n","          loss_D_B_real = self.criterion_GAN(pred_real, self.target_real)\n","          loss_D_B_real_cam = self.criterion_GAN(pred_real_cam, \n","                                               (torch.ones_like(pred_real_cam) * REAL_LABEL).to(device))\n","          \n","          # Fake loss\n","          loss_D_B_fake = self.criterion_GAN(pred_fake, self.target_fake)\n","          loss_D_B_fake_cam = self.criterion_GAN(pred_fake_cam,\n","                                                torch.zeros_like(pred_fake_cam).to(device))\n","          \n","          # Total loss. Decide whether to add 0.5\n","          loss_D_B = coef_adv * 0.5 * (loss_D_B_real + loss_D_B_real_cam + loss_D_B_fake + loss_D_B_fake_cam)\n","          time_DB_forward = time.time() - start_time\n","\n","          start_time = time.time()\n","\n","          loss_D_B.backward()\n","          self.optimizer_D_B.step()\n","\n","          time_DB_backward = time.time() - start_time\n","          ###################################\n","\n","          step += 1\n","          if step % img_snapshot == 0:\n","            img_fake_A = 0.5 * (fake_B2A.detach().data + 1.0)\n","            img_fake_A = (to_pil(img_fake_A.data.squeeze(0).cpu()))\n","            img_fake_A.save(os.path.join(images_dir, f\"fake_A_{step}.png\"))\n","\n","            img_fake_B = 0.5 * (fake_A2B.detach().data + 1.0)\n","            img_fake_B = (to_pil(img_fake_B.data.squeeze(0).cpu()))\n","            img_fake_B.save(os.path.join(images_dir, f\"fake_B_{step}.png\"))\n","\n","          # logging\n","          if step % log_snapshot == 0:\n","            # generator\n","            self.writer.add_scalar('G/GAN_A2B', loss_GAN_A2B, global_step=step)\n","            self.writer.add_scalar('G/GAN_B2A', loss_GAN_B2A, global_step=step)\n","            self.writer.add_scalar('G/cycle_ABA', loss_cycle_ABA, global_step=step)\n","            self.writer.add_scalar('G/cycle_BAB', loss_cycle_BAB, global_step=step)\n","            self.writer.add_scalar('G/idt_A', loss_identity_A, global_step=step)\n","            self.writer.add_scalar('G/idt_B', loss_identity_B, global_step=step)\n","            self.writer.add_scalar('G/cam_A2B', loss_cam_A2B, global_step=step)\n","            self.writer.add_scalar('G/cam_B2A', loss_cam_B2A, global_step=step)\n","\n","            # discriminator\n","            self.writer.add_scalar('D/D_A', loss_D_A, global_step=step)\n","            self.writer.add_scalar('D/D_B', loss_D_B, global_step=step)\n","\n","            self.writer.add_scalar('D/D_A_real', loss_D_A_real, global_step=step)\n","            self.writer.add_scalar('D/D_A_real_cam', loss_D_A_real_cam, global_step=step)\n","            self.writer.add_scalar('D/D_A_fake', loss_D_A_fake, global_step=step)\n","            self.writer.add_scalar('D/D_A_fake_cam', loss_D_A_fake_cam, global_step=step)\n","\n","            self.writer.add_scalar('D/D_B_real', loss_D_B_real, global_step=step)\n","            self.writer.add_scalar('D/D_B_real_cam', loss_D_B_real_cam, global_step=step)\n","            self.writer.add_scalar('D/D_B_fake', loss_D_B_fake, global_step=step)\n","            self.writer.add_scalar('D/D_B_fake_cam', loss_D_B_fake_cam, global_step=step)\n","\n","          # validation\n","          if step % val_snapshot == 0:\n","            val_input_A = self.Tensor(1, 3, image_size, image_size)\n","            val_input_B = self.Tensor(1, 3, image_size, image_size)\n","            val_input_mask = self.Tensor(1, 1, image_size, image_size)\n","            self.netG_A2B.eval()\n","            scores_whole, scores_region = [],[]\n","            mask_scores = []\n","            for batch_val in self.dataloader_val:\n","              real_A = Variable(val_input_A.copy_(batch_val['A']))\n","              real_B = Variable(val_input_B.copy_(batch_val['B']))\n","              real_mask = Variable(val_input_mask.copy_(batch_val['mask']))\n","\n","              pred_B,_,_ = self.netG_A2B(real_A)\n","              pred_mask = mask_generator(real_A, pred_B)\n","\n","              \n","              mask_iou = iou_coef(real_mask, pred_mask, smooth=1)\n","              score_whole, score_region = evaluate(real_B, pred_B, real_mask)\n","\n","              mask_scores.append(mask_iou)\n","              scores_whole.append(score_whole)\n","              scores_region.append(score_region)\n","\n","            self.writer.add_scalar('Score/val_rmse_whole', np.mean(scores_whole), global_step=step)\n","            self.writer.add_scalar('Score/val_rmse_region', np.mean(scores_region), global_step=step)\n","            self.writer.add_scalar('Score/val_iou', np.mean(mask_scores), global_step=step)\n","\n","            self.netG_A2B.train()\n","\n","\n","\n","        # Update learning rates\n","        self.lr_scheduler_G.step()\n","        self.lr_scheduler_D_A.step()\n","        self.lr_scheduler_D_B.step()\n","\n","\n","        # Save models checkpoints\n","        if ep % model_snapshot == 0:\n","          print(f\"Save model -- epoch: {ep}, step: {step}\")\n","          self.save_model(ep, step)\n","\n","          print('Run validation...')\n","          save_test(A_dir_inf, B_dir_inf, os.path.join(results_dir, f'msg_{ep}'))\n","          \n","\n","        epoch_cp = ep\n","\n","\n","    except Exception as e:\n","      raise e\n","    finally:\n","      print(f\"Save model -- epoch: {epoch_cp}, step: {step}\")\n","      self.save_model(epoch_cp, step)\n","\n","      print('Run validation...')\n","      save_test(A_dir_inf, B_dir_inf, os.path.join(results_dir, f'msg_{epoch_cp}'))\n","\n","  def _get_param_by_name(self, model, param_name):\n","    params = [x[1] for x in model.named_parameters() if param_name in x[0]]\n","    return {i:val for (i,val) in enumerate(params)}\n","\n","  def save_model(self, epoch_num, step):\n","    np.savetxt(os.path.join(checkpoint_dir, 'epoch_num.txt'), [epoch_num])\n","    np.savetxt(os.path.join(checkpoint_dir, 'step.txt'), [step])\n","\n","    torch.save(self.netG_A2B.state_dict(), os.path.join(checkpoint_dir, 'netG_A2B.pth'))\n","    torch.save(self.netG_B2A.state_dict(), os.path.join(checkpoint_dir, 'netG_B2A.pth'))\n","    torch.save(self.netD_A.state_dict(), os.path.join(checkpoint_dir, 'netD_A.pth'))\n","    torch.save(self.netD_B.state_dict(), os.path.join(checkpoint_dir, 'netD_B.pth'))\n","\n","    torch.save(self.optimizer_G.state_dict(), os.path.join(checkpoint_dir, 'optimizer_G.pth'))\n","    torch.save(self.optimizer_D_A.state_dict(), os.path.join(checkpoint_dir, 'optimizer_D_A.pth'))\n","    torch.save(self.optimizer_D_B.state_dict(), os.path.join(checkpoint_dir, 'optimizer_D_B.pth'))\n","\n","    torch.save(self.lr_scheduler_G.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_G.pth'))\n","    torch.save(self.lr_scheduler_D_A.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_D_A.pth'))\n","    torch.save(self.lr_scheduler_D_B.state_dict(), os.path.join(checkpoint_dir, 'lr_scheduler_D_B.pth'))\n","\n","\n","  def load_model(self):\n","    epoch = int(np.loadtxt(os.path.join(checkpoint_dir, 'epoch_num.txt')))\n","    step  = int(np.loadtxt(os.path.join(checkpoint_dir, 'step.txt')))\n","    \n","    self.netG_A2B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_A2B.pth')))\n","    self.netG_B2A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netG_B2A.pth')))\n","    self.netD_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'netD_A.pth')))\n","    self.netD_B.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'netD_B.pth'))))\n","\n","    self.optimizer_G.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'optimizer_G.pth'))))\n","    self.optimizer_D_A.load_state_dict(torch.load((os.path.join(checkpoint_dir, 'optimizer_D_A.pth'))))\n","    self.optimizer_D_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'optimizer_D_B.pth')))\n","\n","    self.lr_scheduler_G.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_G.pth')))\n","    self.lr_scheduler_D_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_D_A.pth')))\n","    self.lr_scheduler_D_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'lr_scheduler_D_B.pth')))\n","    print(f\"Model loaded -- {epoch}:{step}\")\n","    return epoch, step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgVInVtEI5es","colab_type":"code","outputId":"5258d18a-c32d-47cd-fe77-3934b7b940a1","executionInfo":{"status":"ok","timestamp":1585240618785,"user_tz":-120,"elapsed":74439,"user":{"displayName":"Alla Andronik","photoUrl":"","userId":"03145443786364096725"}},"colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["model = MaskShadowGAN().build()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["------------------- Definition of variables -------------------\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"_H8dyz8JJGE1","colab_type":"code","outputId":"c5e8128f-a690-4223-80d3-e93d67e5f671","executionInfo":{"status":"error","timestamp":1584985201912,"user_tz":-120,"elapsed":2193917,"user":{"displayName":"Владислав Андроник","photoUrl":"","userId":"15929840215236502543"}},"colab":{"base_uri":"https://localhost:8080/","height":155,"referenced_widgets":["3deacaa13aa640a38417a32a3c8493cd","db1918093f2240819284aeb2fd80cc79","70aee17312f140cda91736929bc2875a","bee6680cd10a451d952e8fecaca2c8ca","f654cf10647747d0bf604068c016d5cb","31771332f6034daab06a6ac8b86b16a0","1d91411aad064c51996987343b039897","ea350a51e8d54b9286e22a84af9a607d"]}},"source":["model.train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["------------------- Start Training -------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3deacaa13aa640a38417a32a3c8493cd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1JIndGMT7f6t","colab_type":"code","colab":{}},"source":["\"\"\"\n","function preventFromOff(){\n","  console.log(\"Click button\");\n","  document.querySelector(\"colab-connect-button\").shadowRoot.getElementById(\"connect\").click()\n","}\n","var timeout = 8 * 60 * 60 * 1000;\n","var delay = 3 * 60 * 1000;\n","var refreshId = setInterval(preventFromOff, delay);\n","setTimeout(() => {clearInterval(refreshId); console.log(\"Stopped script\");}, timeout);\n","\"\"\""],"execution_count":0,"outputs":[]}]}